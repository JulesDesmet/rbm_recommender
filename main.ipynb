{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter==1.0.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: numpy==1.21.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.21.3)\r\n",
      "Requirement already satisfied: matplotlib==3.4.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.4.3)\r\n",
      "Requirement already satisfied: pandas==1.3.4 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.3.4)\r\n",
      "Requirement already satisfied: scikit-learn==1.0.1 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.0.1)\r\n",
      "Requirement already satisfied: torch==1.10.0 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.10.0)\r\n",
      "Requirement already satisfied: tqdm==4.62.3 in ./venv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.62.3)\r\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (7.6.5)\r\n",
      "Requirement already satisfied: ipykernel in ./venv/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.1)\r\n",
      "Requirement already satisfied: qtconsole in ./venv/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: nbconvert in ./venv/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.2.0)\r\n",
      "Requirement already satisfied: notebook in ./venv/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.4)\r\n",
      "Requirement already satisfied: jupyter-console in ./venv/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.7.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/jules/.local/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jules/.local/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jules/.local/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (0.10.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jules/.local/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (8.2.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas==1.3.4->-r requirements.txt (line 4)) (2019.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jules/.local/lib/python3.9/site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/jules/.local/lib/python3.9/site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (1.6.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/jules/.local/lib/python3.9/site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (0.17.0)\r\n",
      "Requirement already satisfied: typing-extensions in /home/jules/.local/lib/python3.9/site-packages (from torch==1.10.0->-r requirements.txt (line 6)) (3.7.4.3)\r\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib==3.4.3->-r requirements.txt (line 3)) (1.14.0)\r\n",
      "Requirement already satisfied: ipython-genutils in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.0)\r\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.3)\r\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (6.1)\r\n",
      "Requirement already satisfied: jupyter-client<8.0 in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (7.0.6)\r\n",
      "Requirement already satisfied: traitlets<6.0,>=4.1.0 in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.0)\r\n",
      "Requirement already satisfied: ipython<8.0,>=7.23.1 in ./venv/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (7.28.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in ./venv/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (3.5.1)\r\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in ./venv/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (1.0.2)\r\n",
      "Requirement already satisfied: nbformat>=4.2.0 in ./venv/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.3)\r\n",
      "Requirement already satisfied: pygments in ./venv/lib/python3.9/site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (2.10.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in ./venv/lib/python3.9/site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.20)\r\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.4)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.0)\r\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/lib/python3/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (2.10.1)\r\n",
      "Requirement already satisfied: testpath in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.0)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.4)\r\n",
      "Requirement already satisfied: defusedxml in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: jupyter-core in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (4.8.1)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/lib/python3/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.3)\r\n",
      "Requirement already satisfied: bleach in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (4.1.0)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in ./venv/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.2)\r\n",
      "Requirement already satisfied: pyzmq>=17 in ./venv/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (22.3.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./venv/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.1)\r\n",
      "Requirement already satisfied: argon2-cffi in ./venv/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (21.1.0)\r\n",
      "Requirement already satisfied: prometheus-client in ./venv/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.11.0)\r\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in ./venv/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: qtpy in ./venv/lib/python3.9/site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (1.11.2)\r\n",
      "Requirement already satisfied: backcall in ./venv/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (4.6.0)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/jules/.local/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.17.2)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in ./venv/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (58.1.0)\r\n",
      "Requirement already satisfied: pickleshare in ./venv/lib/python3.9/site-packages (from ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.5)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in ./venv/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in ./venv/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (4.1.0)\r\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.5)\r\n",
      "Requirement already satisfied: ptyprocess in ./venv/lib/python3.9/site-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: cffi>=1.0.0 in ./venv/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.14.6)\r\n",
      "Requirement already satisfied: webencodings in ./venv/lib/python3.9/site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.9/site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (21.0)\r\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.20)\r\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/jules/.local/lib/python3.9/site-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./venv/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (21.2.0)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in ./venv/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.18.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/jules/Documents/rbm_recommender/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from json import loads\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display  # removes unnecessary error reports in PyCharm\n",
    "from scipy import sparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def parse_json(filename: str, read_max: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the file line by line, parsing each line as json.\n",
    "    \"\"\"\n",
    "    file = gzip.open(filename, \"r\")\n",
    "    data = []\n",
    "    for index, line in enumerate(tqdm(file)):\n",
    "        if index == read_max:\n",
    "            break\n",
    "        data.append(loads(line))\n",
    "    print(f\"Read {len(data)} rows.\")\n",
    "    return pd.DataFrame.from_dict(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset consists of three separate files:\n",
    "\n",
    "- The books; we will only need this data to make sense of the recommendations. Using this data to make recommendations\n",
    "is (at least for now) not required for the project. It would require analysis and comparisons of the books, which is not\n",
    "a part of the base algorithm. However, being able to show which books are being recommended, rather than just showing an\n",
    "ID, is quite valuable in itself.\n",
    "\n",
    "- The reviews; this file contains all the reviews and associated ratings that the users have given. This is essentially\n",
    "the explicit feedback that we can use to generate recommendations.\n",
    "\n",
    "- The interactions; this file contains all the interactions between users and books. It contains explicit and implicit\n",
    "feedback, both of which we can use to generate recommendations. We will probably only use the implicit data if we do use\n",
    "the data in this file.\n",
    "\n",
    "We have the option of using either explicit (i.e. ratings) or implicit (i.e. interactions) data. Because the paper\n",
    "discusses the prediction of ratings, this is also what we will be doing.\n",
    "\n",
    "The following cells load the data from the files and convert them into the appropriate types. This includes parsing\n",
    "datetime strings, converting integers to numpy types, etc.\n",
    "\n",
    "## Important variables/settings\n",
    "\n",
    "- *n* determines the maximum number of rows read from any of the files.\n",
    "- *datafile*, which can be either `interactions` or `reviews`, determines the file from which the data matrix will be\n",
    "read.\n",
    "- *k* determines the number of predictions the recommender will make, and on how many predictions it will be evaluated,\n",
    "e.g. by using `Recall@k`.\n",
    "- *epochs* determines the number of epochs we will use to train the recommender."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodreads_books_comics_graphic.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4239b1ca4ad34283a657a1c2ab5f9e68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 89411 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": "         isbn text_reviews_count                            series  \\\n0                              1                                []   \n1  2205073346                  2                                []   \n2                              5  [246830, 362583, 362581, 623032]   \n3                              1                                []   \n4  0930289765                  6                 [266759, 1096220]   \n\n  country_code language_code  \\\n0           US                 \n1           US           fre   \n2           US           eng   \n3           US           eng   \n4           US         en-US   \n\n                                     popular_shelves        asin is_ebook  \\\n0  [{'count': '228', 'name': 'to-read'}, {'count'...  B00NLXQ534     true   \n1  [{'count': '2', 'name': 'bd'}, {'count': '2', ...                false   \n2  [{'count': '493', 'name': 'to-read'}, {'count'...                false   \n3  [{'count': '222', 'name': 'to-read'}, {'count'...  B06XKGGSB7     true   \n4  [{'count': '20', 'name': 'to-read'}, {'count':...                false   \n\n  average_rating kindle_asin  ... publication_month edition_information  \\\n0           4.12              ...                                         \n1           3.94              ...                 1                       \n2           4.28              ...                                         \n3           4.05  B06XKGGSB7  ...                                         \n4           4.06              ...                11                       \n\n  publication_year                                                url  \\\n0                   https://www.goodreads.com/book/show/25742454-t...   \n1             2016  https://www.goodreads.com/book/show/30128855-c...   \n2             2012  https://www.goodreads.com/book/show/13571772-c...   \n3                   https://www.goodreads.com/book/show/35452242-b...   \n4             1997  https://www.goodreads.com/book/show/707611.Sup...   \n\n                                           image_url   book_id ratings_count  \\\n0  https://s.gr-assets.com/assets/nophoto/book/11...  25742454             1   \n1  https://images.gr-assets.com/books/1462644346m...  30128855            16   \n2  https://images.gr-assets.com/books/1333287305m...  13571772            51   \n3  https://s.gr-assets.com/assets/nophoto/book/11...  35452242             6   \n4  https://images.gr-assets.com/books/1307838888m...    707611            51   \n\n    work_id                                              title  \\\n0  42749946                              The Switchblade Mamma   \n1  50558228                                            Cruelle   \n2    102217  Captain America: Winter Soldier (The Ultimate ...   \n3  54276229  Bounty Hunter 4/3: My Life in Combat from Mari...   \n4    693886                          Superman Archives, Vol. 2   \n\n                                title_without_series  \n0                              The Switchblade Mamma  \n1                                            Cruelle  \n2  Captain America: Winter Soldier (The Ultimate ...  \n3  Bounty Hunter 4/3: My Life in Combat from Mari...  \n4                          Superman Archives, Vol. 2  \n\n[5 rows x 29 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isbn</th>\n      <th>text_reviews_count</th>\n      <th>series</th>\n      <th>country_code</th>\n      <th>language_code</th>\n      <th>popular_shelves</th>\n      <th>asin</th>\n      <th>is_ebook</th>\n      <th>average_rating</th>\n      <th>kindle_asin</th>\n      <th>...</th>\n      <th>publication_month</th>\n      <th>edition_information</th>\n      <th>publication_year</th>\n      <th>url</th>\n      <th>image_url</th>\n      <th>book_id</th>\n      <th>ratings_count</th>\n      <th>work_id</th>\n      <th>title</th>\n      <th>title_without_series</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>1</td>\n      <td>[]</td>\n      <td>US</td>\n      <td></td>\n      <td>[{'count': '228', 'name': 'to-read'}, {'count'...</td>\n      <td>B00NLXQ534</td>\n      <td>true</td>\n      <td>4.12</td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>https://www.goodreads.com/book/show/25742454-t...</td>\n      <td>https://s.gr-assets.com/assets/nophoto/book/11...</td>\n      <td>25742454</td>\n      <td>1</td>\n      <td>42749946</td>\n      <td>The Switchblade Mamma</td>\n      <td>The Switchblade Mamma</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2205073346</td>\n      <td>2</td>\n      <td>[]</td>\n      <td>US</td>\n      <td>fre</td>\n      <td>[{'count': '2', 'name': 'bd'}, {'count': '2', ...</td>\n      <td></td>\n      <td>false</td>\n      <td>3.94</td>\n      <td></td>\n      <td>...</td>\n      <td>1</td>\n      <td></td>\n      <td>2016</td>\n      <td>https://www.goodreads.com/book/show/30128855-c...</td>\n      <td>https://images.gr-assets.com/books/1462644346m...</td>\n      <td>30128855</td>\n      <td>16</td>\n      <td>50558228</td>\n      <td>Cruelle</td>\n      <td>Cruelle</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>5</td>\n      <td>[246830, 362583, 362581, 623032]</td>\n      <td>US</td>\n      <td>eng</td>\n      <td>[{'count': '493', 'name': 'to-read'}, {'count'...</td>\n      <td></td>\n      <td>false</td>\n      <td>4.28</td>\n      <td></td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td>2012</td>\n      <td>https://www.goodreads.com/book/show/13571772-c...</td>\n      <td>https://images.gr-assets.com/books/1333287305m...</td>\n      <td>13571772</td>\n      <td>51</td>\n      <td>102217</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td>1</td>\n      <td>[]</td>\n      <td>US</td>\n      <td>eng</td>\n      <td>[{'count': '222', 'name': 'to-read'}, {'count'...</td>\n      <td>B06XKGGSB7</td>\n      <td>true</td>\n      <td>4.05</td>\n      <td>B06XKGGSB7</td>\n      <td>...</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>https://www.goodreads.com/book/show/35452242-b...</td>\n      <td>https://s.gr-assets.com/assets/nophoto/book/11...</td>\n      <td>35452242</td>\n      <td>6</td>\n      <td>54276229</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0930289765</td>\n      <td>6</td>\n      <td>[266759, 1096220]</td>\n      <td>US</td>\n      <td>en-US</td>\n      <td>[{'count': '20', 'name': 'to-read'}, {'count':...</td>\n      <td></td>\n      <td>false</td>\n      <td>4.06</td>\n      <td></td>\n      <td>...</td>\n      <td>11</td>\n      <td></td>\n      <td>1997</td>\n      <td>https://www.goodreads.com/book/show/707611.Sup...</td>\n      <td>https://images.gr-assets.com/books/1307838888m...</td>\n      <td>707611</td>\n      <td>51</td>\n      <td>693886</td>\n      <td>Superman Archives, Vol. 2</td>\n      <td>Superman Archives, Vol. 2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodreads_interactions_comics_graphic.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a8e581f894d4d46ad62f7b4fc6aa315"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 500000 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            user_id   book_id  \\\n0  8842281e1d1347389f2ab93d60773d4d    836610   \n1  8842281e1d1347389f2ab93d60773d4d   7648967   \n2  8842281e1d1347389f2ab93d60773d4d  15704307   \n3  8842281e1d1347389f2ab93d60773d4d   6902644   \n4  8842281e1d1347389f2ab93d60773d4d   9844623   \n\n                          review_id  is_read  rating review_text_incomplete  \\\n0  6b4db26aafeaf0da77c7de6214331e1e    False       0                          \n1  99b27059f711c37de8f90ee8e4dc0d1b    False       0                          \n2  cb944d94854df5afd22210bb0aa0c903    False       0                          \n3  2711bac2a8cc600dae1590a6ca0edb34    False       0                          \n4  b72979076d1cded25dded922195e5b1c    False       0                          \n\n                       date_added                    date_updated read_at  \\\n0  Mon Aug 21 12:11:00 -0700 2017  Mon Aug 21 12:11:00 -0700 2017           \n1  Fri Feb 24 08:59:44 -0800 2017  Fri Feb 24 08:59:44 -0800 2017           \n2  Wed May 20 21:28:56 -0700 2015  Wed May 20 21:28:57 -0700 2015           \n3  Sun Jun 01 17:25:23 -0700 2014  Sun Jun 01 17:25:23 -0700 2014           \n4  Sun Sep 02 08:45:08 -0700 2012  Sun Sep 02 08:45:08 -0700 2012           \n\n  started_at  \n0             \n1             \n2             \n3             \n4             ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>is_read</th>\n      <th>rating</th>\n      <th>review_text_incomplete</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>6b4db26aafeaf0da77c7de6214331e1e</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td>Mon Aug 21 12:11:00 -0700 2017</td>\n      <td>Mon Aug 21 12:11:00 -0700 2017</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>99b27059f711c37de8f90ee8e4dc0d1b</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td>Fri Feb 24 08:59:44 -0800 2017</td>\n      <td>Fri Feb 24 08:59:44 -0800 2017</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>cb944d94854df5afd22210bb0aa0c903</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td>Wed May 20 21:28:56 -0700 2015</td>\n      <td>Wed May 20 21:28:57 -0700 2015</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>2711bac2a8cc600dae1590a6ca0edb34</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td>Sun Jun 01 17:25:23 -0700 2014</td>\n      <td>Sun Jun 01 17:25:23 -0700 2014</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>b72979076d1cded25dded922195e5b1c</td>\n      <td>False</td>\n      <td>0</td>\n      <td></td>\n      <td>Sun Sep 02 08:45:08 -0700 2012</td>\n      <td>Sun Sep 02 08:45:08 -0700 2012</td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodreads_reviews_comics_graphic.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1848b9f35ff543b1a37adc62cd4e039b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 500000 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            user_id   book_id  \\\n0  dc3763cdb9b2cae805882878eebb6a32  18471619   \n1  bafc2d50014200cda7cb2b6acd60cd73   6315584   \n2  bafc2d50014200cda7cb2b6acd60cd73  29847729   \n3  bafc2d50014200cda7cb2b6acd60cd73  18454118   \n4  bafc2d50014200cda7cb2b6acd60cd73   2239435   \n\n                          review_id  rating  \\\n0  66b2ba840f9bd36d6d27f46136fe4772       3   \n1  72f1229aba5a88f9e72f0dcdc007dd22       4   \n2  a75309355f8662caaa5e2c92ab693d3f       4   \n3  c3cc5a3e1d6b6c9cf1c044f306c8e752       5   \n4  cc444be37ab0a42bfb4dd818cb5edd10       4   \n\n                                         review_text  \\\n0  Sherlock Holmes and the Vampires of London \\n ...   \n1  I've never really liked Spider-Man. I am, howe...   \n2  A very quick introduction, this is coming out ...   \n3  I've been waiting so long for this. I first st...   \n4  The only thing more entertaining than this boo...   \n\n                       date_added                    date_updated  \\\n0  Thu Dec 05 10:44:25 -0800 2013  Thu Dec 05 10:45:15 -0800 2013   \n1  Wed Aug 10 06:06:48 -0700 2016  Fri Aug 12 08:49:54 -0700 2016   \n2  Thu Apr 21 07:44:00 -0700 2016  Thu Apr 21 07:59:28 -0700 2016   \n3  Mon Mar 03 17:45:56 -0800 2014  Mon Mar 03 17:54:11 -0800 2014   \n4  Wed Apr 03 12:37:48 -0700 2013  Wed Apr 03 13:03:36 -0700 2013   \n\n                          read_at                      started_at  n_votes  \\\n0  Tue Nov 05 00:00:00 -0800 2013                                        0   \n1  Fri Aug 12 08:49:54 -0700 2016  Wed Aug 10 00:00:00 -0700 2016        0   \n2  Thu Apr 21 07:59:28 -0700 2016  Thu Apr 21 00:00:00 -0700 2016        0   \n3  Sat Mar 01 00:00:00 -0800 2014  Sat Mar 01 00:00:00 -0800 2014        1   \n4  Wed Apr 03 13:03:36 -0700 2013                                        0   \n\n   n_comments  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>rating</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dc3763cdb9b2cae805882878eebb6a32</td>\n      <td>18471619</td>\n      <td>66b2ba840f9bd36d6d27f46136fe4772</td>\n      <td>3</td>\n      <td>Sherlock Holmes and the Vampires of London \\n ...</td>\n      <td>Thu Dec 05 10:44:25 -0800 2013</td>\n      <td>Thu Dec 05 10:45:15 -0800 2013</td>\n      <td>Tue Nov 05 00:00:00 -0800 2013</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bafc2d50014200cda7cb2b6acd60cd73</td>\n      <td>6315584</td>\n      <td>72f1229aba5a88f9e72f0dcdc007dd22</td>\n      <td>4</td>\n      <td>I've never really liked Spider-Man. I am, howe...</td>\n      <td>Wed Aug 10 06:06:48 -0700 2016</td>\n      <td>Fri Aug 12 08:49:54 -0700 2016</td>\n      <td>Fri Aug 12 08:49:54 -0700 2016</td>\n      <td>Wed Aug 10 00:00:00 -0700 2016</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bafc2d50014200cda7cb2b6acd60cd73</td>\n      <td>29847729</td>\n      <td>a75309355f8662caaa5e2c92ab693d3f</td>\n      <td>4</td>\n      <td>A very quick introduction, this is coming out ...</td>\n      <td>Thu Apr 21 07:44:00 -0700 2016</td>\n      <td>Thu Apr 21 07:59:28 -0700 2016</td>\n      <td>Thu Apr 21 07:59:28 -0700 2016</td>\n      <td>Thu Apr 21 00:00:00 -0700 2016</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bafc2d50014200cda7cb2b6acd60cd73</td>\n      <td>18454118</td>\n      <td>c3cc5a3e1d6b6c9cf1c044f306c8e752</td>\n      <td>5</td>\n      <td>I've been waiting so long for this. I first st...</td>\n      <td>Mon Mar 03 17:45:56 -0800 2014</td>\n      <td>Mon Mar 03 17:54:11 -0800 2014</td>\n      <td>Sat Mar 01 00:00:00 -0800 2014</td>\n      <td>Sat Mar 01 00:00:00 -0800 2014</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bafc2d50014200cda7cb2b6acd60cd73</td>\n      <td>2239435</td>\n      <td>cc444be37ab0a42bfb4dd818cb5edd10</td>\n      <td>4</td>\n      <td>The only thing more entertaining than this boo...</td>\n      <td>Wed Apr 03 12:37:48 -0700 2013</td>\n      <td>Wed Apr 03 13:03:36 -0700 2013</td>\n      <td>Wed Apr 03 13:03:36 -0700 2013</td>\n      <td></td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"data/\"\n",
    "books = \"goodreads_books_comics_graphic.json.gz\"\n",
    "interactions = \"goodreads_interactions_comics_graphic.json.gz\"\n",
    "reviews = \"goodreads_reviews_comics_graphic.json.gz\"\n",
    "datasets = {books: None, interactions: None, reviews: None}\n",
    "\n",
    "n = 500000\n",
    "datafile = reviews\n",
    "k = 10\n",
    "epochs = 20\n",
    "\n",
    "for filename in datasets.keys():\n",
    "    print(filename)\n",
    "    datasets[filename] = parse_json(data_path + filename, n)\n",
    "    display(datasets[filename].head(5))\n",
    "\n",
    "books_df = datasets[books][[\"book_id\", \"title\"]].copy()\n",
    "interactions_df = datasets[datafile][[\"user_id\", \"book_id\", \"rating\", \"date_updated\"]].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "format_str = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "interactions_df[\"date_updated\"] = pd.to_datetime(interactions_df[\"date_updated\"], format=format_str)\n",
    "\n",
    "books_df[\"book_id\"] = books_df[\"book_id\"].astype(\"int64\")\n",
    "interactions_df[\"book_id\"] = interactions_df[\"book_id\"].astype(\"int64\")\n",
    "\n",
    "interactions_df = interactions_df.sort_values(by=[\"user_id\", \"date_updated\"], ascending=[True, True])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "We apply the following preprocessing steps:\n",
    "\n",
    "- Reconsumption item removal, although there aren't many (or even none at all) of these in the dataset.\n",
    "\n",
    "- Infrequent item removal; we remove any items that have less than a certain number (5) of interactions/ratings.\n",
    "\n",
    "- Infrequent user removal; we remove users with less that the same certain number (5) of interactions/ratings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                 user_id   book_id  rating  \\\n498184  00009e46d18f223a82b22da38586b605  23546829       3   \n54004   000192962b87d560f00b06fdcbd71681  30025791       5   \n255717  0005a08accd53b1e19c52109a1f478cb   1472402       5   \n255716  0005a08accd53b1e19c52109a1f478cb    119162       4   \n255715  0005a08accd53b1e19c52109a1f478cb   3285607       5   \n255714  0005a08accd53b1e19c52109a1f478cb     59960       0   \n255713  0005a08accd53b1e19c52109a1f478cb    107037       3   \n191681  0006260f85929db85eddee3a0bd0e504  18759090       5   \n191680  0006260f85929db85eddee3a0bd0e504  24069836       4   \n191679  0006260f85929db85eddee3a0bd0e504  24685138       5   \n\n                     date_updated  \n498184  2016-01-15 19:31:35-08:00  \n54004   2017-01-28 09:56:08-08:00  \n255717  2007-09-16 21:21:34-07:00  \n255716  2008-04-11 01:09:21-07:00  \n255715  2008-05-12 19:27:12-07:00  \n255714  2008-09-09 18:23:53-07:00  \n255713  2011-07-20 10:58:49-07:00  \n191681  2015-05-06 09:29:17-07:00  \n191680  2015-07-04 23:27:08-07:00  \n191679  2015-07-20 04:54:10-07:00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>date_updated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>498184</th>\n      <td>00009e46d18f223a82b22da38586b605</td>\n      <td>23546829</td>\n      <td>3</td>\n      <td>2016-01-15 19:31:35-08:00</td>\n    </tr>\n    <tr>\n      <th>54004</th>\n      <td>000192962b87d560f00b06fdcbd71681</td>\n      <td>30025791</td>\n      <td>5</td>\n      <td>2017-01-28 09:56:08-08:00</td>\n    </tr>\n    <tr>\n      <th>255717</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>1472402</td>\n      <td>5</td>\n      <td>2007-09-16 21:21:34-07:00</td>\n    </tr>\n    <tr>\n      <th>255716</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>119162</td>\n      <td>4</td>\n      <td>2008-04-11 01:09:21-07:00</td>\n    </tr>\n    <tr>\n      <th>255715</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>3285607</td>\n      <td>5</td>\n      <td>2008-05-12 19:27:12-07:00</td>\n    </tr>\n    <tr>\n      <th>255714</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>59960</td>\n      <td>0</td>\n      <td>2008-09-09 18:23:53-07:00</td>\n    </tr>\n    <tr>\n      <th>255713</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>107037</td>\n      <td>3</td>\n      <td>2011-07-20 10:58:49-07:00</td>\n    </tr>\n    <tr>\n      <th>191681</th>\n      <td>0006260f85929db85eddee3a0bd0e504</td>\n      <td>18759090</td>\n      <td>5</td>\n      <td>2015-05-06 09:29:17-07:00</td>\n    </tr>\n    <tr>\n      <th>191680</th>\n      <td>0006260f85929db85eddee3a0bd0e504</td>\n      <td>24069836</td>\n      <td>4</td>\n      <td>2015-07-04 23:27:08-07:00</td>\n    </tr>\n    <tr>\n      <th>191679</th>\n      <td>0006260f85929db85eddee3a0bd0e504</td>\n      <td>24685138</td>\n      <td>5</td>\n      <td>2015-07-20 04:54:10-07:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 50475\n",
      "Number of unique items: 86262\n",
      "500000 initial rows\n",
      "500000 rows after removing reconsumption items\n",
      "339034 rows after removing infrequent items and users\n",
      "Number of unique users: 14973\n",
      "Number of unique items: 17869\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             user_id   book_id  rating  \\\n2   0005a08accd53b1e19c52109a1f478cb   1472402       5   \n4   0005a08accd53b1e19c52109a1f478cb   3285607       5   \n5   0005a08accd53b1e19c52109a1f478cb     59960       0   \n10  0006260f85929db85eddee3a0bd0e504  29869650       4   \n11  0006260f85929db85eddee3a0bd0e504  29521992       5   \n17  0008931c0cde961e9c802c5a58196d23   6081685       5   \n18  0008931c0cde961e9c802c5a58196d23   7311068       5   \n19  0008931c0cde961e9c802c5a58196d23   6690979       5   \n21  0008931c0cde961e9c802c5a58196d23   6599344       5   \n22  0008931c0cde961e9c802c5a58196d23   6372263       5   \n\n                 date_updated  \n2   2007-09-16 21:21:34-07:00  \n4   2008-05-12 19:27:12-07:00  \n5   2008-09-09 18:23:53-07:00  \n10  2016-05-11 04:53:09-07:00  \n11  2016-10-07 05:08:26-07:00  \n17  2013-03-09 07:02:48-08:00  \n18  2013-03-09 07:03:42-08:00  \n19  2013-03-09 07:05:26-08:00  \n21  2014-01-26 10:16:17-08:00  \n22  2014-01-26 10:19:19-08:00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>date_updated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>1472402</td>\n      <td>5</td>\n      <td>2007-09-16 21:21:34-07:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>3285607</td>\n      <td>5</td>\n      <td>2008-05-12 19:27:12-07:00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0005a08accd53b1e19c52109a1f478cb</td>\n      <td>59960</td>\n      <td>0</td>\n      <td>2008-09-09 18:23:53-07:00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0006260f85929db85eddee3a0bd0e504</td>\n      <td>29869650</td>\n      <td>4</td>\n      <td>2016-05-11 04:53:09-07:00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0006260f85929db85eddee3a0bd0e504</td>\n      <td>29521992</td>\n      <td>5</td>\n      <td>2016-10-07 05:08:26-07:00</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0008931c0cde961e9c802c5a58196d23</td>\n      <td>6081685</td>\n      <td>5</td>\n      <td>2013-03-09 07:02:48-08:00</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0008931c0cde961e9c802c5a58196d23</td>\n      <td>7311068</td>\n      <td>5</td>\n      <td>2013-03-09 07:03:42-08:00</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0008931c0cde961e9c802c5a58196d23</td>\n      <td>6690979</td>\n      <td>5</td>\n      <td>2013-03-09 07:05:26-08:00</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0008931c0cde961e9c802c5a58196d23</td>\n      <td>6599344</td>\n      <td>5</td>\n      <td>2014-01-26 10:16:17-08:00</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0008931c0cde961e9c802c5a58196d23</td>\n      <td>6372263</td>\n      <td>5</td>\n      <td>2014-01-26 10:19:19-08:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(df: pd.DataFrame, min_support: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes reconsumption items, items that have less than `min_support` interactions, and users that have less than\n",
    "    `min_support` interactions. In some cases, removing an infrequent item may turn a frequent user into an infrequent\n",
    "    one, and vice versa. In these cases, we don't remove the now infrequent user/item, we only consider the original\n",
    "    frequency. As such, the preprocessed dataset may contain some users and items that don't reach the minimum support\n",
    "    limit.\n",
    "    \"\"\"\n",
    "    print(df.shape[0], \"initial rows\")\n",
    "    # Drop reconsumption items\n",
    "    df = df.drop_duplicates(subset=[\"user_id\", \"book_id\"])\n",
    "    print(df.shape[0], \"rows after removing reconsumption items\")\n",
    "    # Compute user and item counts\n",
    "    g1 = df.groupby(\"book_id\", as_index=False)[\"user_id\"].size()\n",
    "    g1 = g1.rename({\"size\": \"users_per_item\"}, axis=\"columns\")\n",
    "    g2 = df.groupby(\"user_id\", as_index=False)[\"book_id\"].size()\n",
    "    g2 = g2.rename({\"size\": \"items_per_user\"}, axis=\"columns\")\n",
    "    df = pd.merge(df, g1, how=\"left\", on=[\"book_id\"])\n",
    "    df = pd.merge(df, g2, how=\"left\", on=[\"user_id\"])\n",
    "    # Drop items and users with less than `min_support` interactions\n",
    "    df = df[(df[\"users_per_item\"] >= min_support) & (df[\"items_per_user\"] >= min_support)]\n",
    "    print(df.shape[0], \"rows after removing infrequent items and users\")\n",
    "    df.drop(columns=[\"users_per_item\", \"items_per_user\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "display(interactions_df.head(10))\n",
    "print(f\"Number of unique users:\", interactions_df[\"user_id\"].nunique())\n",
    "print(f\"Number of unique items:\", interactions_df[\"book_id\"].nunique())\n",
    "processed_df = preprocess(interactions_df)\n",
    "print(f\"Number of unique users:\", processed_df[\"user_id\"].nunique())\n",
    "print(f\"Number of unique items:\", processed_df[\"book_id\"].nunique())\n",
    "display(processed_df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/339034 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d73cfc32dde45bf92814acb7c563b12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/89411 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e5900b370ce4349a6fcfb6799f64ff5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/339034 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75bf07ab9ba04065ba6e55d285e5c5cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    book_id                                              title  book_id_int\n0  25742454                              The Switchblade Mamma            0\n1  30128855                                            Cruelle            1\n2  13571772  Captain America: Winter Soldier (The Ultimate ...            2\n3  35452242  Bounty Hunter 4/3: My Life in Combat from Mari...            3\n4    707611                          Superman Archives, Vol. 2            4\n5   2250580                            A.I. Revolution, Vol. 1            5\n6  27036536                              War Stories, Volume 3            6\n7  27036537                                 Crossed, Volume 15            7\n8  27036538  Crossed + One Hundred, Volume 2 (Crossed +100 #2)            8\n9  27036539                              War Stories, Volume 4            9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>title</th>\n      <th>book_id_int</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25742454</td>\n      <td>The Switchblade Mamma</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30128855</td>\n      <td>Cruelle</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13571772</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35452242</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>707611</td>\n      <td>Superman Archives, Vol. 2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2250580</td>\n      <td>A.I. Revolution, Vol. 1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>27036536</td>\n      <td>War Stories, Volume 3</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27036537</td>\n      <td>Crossed, Volume 15</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27036538</td>\n      <td>Crossed + One Hundred, Volume 2 (Crossed +100 #2)</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27036539</td>\n      <td>War Stories, Volume 4</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    user_id  item_id                   datetime  rating\n2         0    55005  2007-09-16 21:21:34-07:00       5\n4         0    61146  2008-05-12 19:27:12-07:00       5\n5         0    47066  2008-09-09 18:23:53-07:00       0\n10        1     6248  2016-05-11 04:53:09-07:00       4\n11        1    67578  2016-10-07 05:08:26-07:00       5\n17        2    44682  2013-03-09 07:02:48-08:00       5\n18        2    85879  2013-03-09 07:03:42-08:00       5\n19        2    50729  2013-03-09 07:05:26-08:00       5\n21        2     4168  2014-01-26 10:16:17-08:00       5\n22        2    14482  2014-01-26 10:19:19-08:00       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>datetime</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>55005</td>\n      <td>2007-09-16 21:21:34-07:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>61146</td>\n      <td>2008-05-12 19:27:12-07:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>47066</td>\n      <td>2008-09-09 18:23:53-07:00</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>6248</td>\n      <td>2016-05-11 04:53:09-07:00</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1</td>\n      <td>67578</td>\n      <td>2016-10-07 05:08:26-07:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2</td>\n      <td>44682</td>\n      <td>2013-03-09 07:02:48-08:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>85879</td>\n      <td>2013-03-09 07:03:42-08:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2</td>\n      <td>50729</td>\n      <td>2013-03-09 07:05:26-08:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>2</td>\n      <td>4168</td>\n      <td>2014-01-26 10:16:17-08:00</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>2</td>\n      <td>14482</td>\n      <td>2014-01-26 10:19:19-08:00</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_consecutive_mapping(dataframe: pd.DataFrame, column: str, new_column: str, *additional: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generates a consecutive ID column for the values of an existing column. Also adds this column to additional data\n",
    "    frames with the exact same mapping of old ID to new (consecutive) ID.\n",
    "    \"\"\"\n",
    "    ids = {}\n",
    "\n",
    "    def map_to_consecutive_ids(uuid: Union[int, np.int64]) -> int:\n",
    "        \"\"\"\n",
    "        To be used with `pd.Dataframe.apply()` or `pd.Dataframe.progress_apply()`; returns a unique ID per distinct\n",
    "        value.\n",
    "        \"\"\"\n",
    "        if uuid not in ids:\n",
    "            ids[uuid] = len(ids)\n",
    "        return ids[uuid]\n",
    "\n",
    "    dataframe[new_column] = dataframe[column].progress_apply(map_to_consecutive_ids)\n",
    "    for frame in additional:\n",
    "        frame[new_column] = frame[column].progress_apply(lambda old_id: ids.get(old_id, -1))\n",
    "\n",
    "\n",
    "apply_consecutive_mapping(processed_df, \"user_id\", \"user_id_int\")\n",
    "apply_consecutive_mapping(books_df, \"book_id\", \"book_id_int\", processed_df)\n",
    "processed_df = processed_df[[\"user_id_int\", \"book_id_int\", \"date_updated\", \"rating\"]]\n",
    "processed_df = processed_df.rename(\n",
    "    columns={\"user_id_int\": \"user_id\", \"book_id_int\": \"item_id\", \"date_updated\": \"datetime\"})\n",
    "\n",
    "display(books_df.head(10))\n",
    "display(processed_df.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def split(items: list[int], percentage_train: float) -> pd.Series:\n",
    "    nr_train_items = int(len(items) * percentage_train)\n",
    "    return pd.Series((items[: nr_train_items], items[nr_train_items:]))\n",
    "\n",
    "\n",
    "sessions_df = processed_df.groupby(by=\"user_id\", as_index=False)[[\"item_id\", \"datetime\", \"rating\"]].agg(list)\n",
    "display(sessions_df.head(5))\n",
    "\n",
    "percentage_train = 0.8\n",
    "sessions_df[[\"item_id_history\", \"item_id_future\"]] = sessions_df[\"item_id\"].progress_apply(split,\n",
    "                                                                                           args=(percentage_train,))\n",
    "sessions_df[[\"rating_history\", \"rating_future\"]] = sessions_df[\"rating\"].progress_apply(split, args=(percentage_train,))\n",
    "display(sessions_df.head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   user_id                                            item_id  \\\n0        0                              [55005, 61146, 47066]   \n1        1                                      [6248, 67578]   \n2        2  [44682, 85879, 50729, 4168, 14482, 78634, 7653...   \n3        3                [18454, 73033, 18962, 22632, 58803]   \n4        4  [37855, 19881, 45252, 83738, 78185, 9713, 9762...   \n\n                                            datetime  \\\n0  [2007-09-16 21:21:34-07:00, 2008-05-12 19:27:1...   \n1  [2016-05-11 04:53:09-07:00, 2016-10-07 05:08:2...   \n2  [2013-03-09 07:02:48-08:00, 2013-03-09 07:03:4...   \n3  [2017-04-04 12:19:56-07:00, 2017-04-06 10:10:5...   \n4  [2016-03-04 06:02:24-08:00, 2016-07-23 01:02:4...   \n\n                                 rating  \n0                             [5, 5, 0]  \n1                                [4, 5]  \n2  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]  \n3                       [5, 5, 5, 5, 5]  \n4  [3, 4, 3, 3, 3, 2, 1, 3, 3, 3, 4, 3]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>datetime</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[55005, 61146, 47066]</td>\n      <td>[2007-09-16 21:21:34-07:00, 2008-05-12 19:27:1...</td>\n      <td>[5, 5, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[6248, 67578]</td>\n      <td>[2016-05-11 04:53:09-07:00, 2016-10-07 05:08:2...</td>\n      <td>[4, 5]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[44682, 85879, 50729, 4168, 14482, 78634, 7653...</td>\n      <td>[2013-03-09 07:02:48-08:00, 2013-03-09 07:03:4...</td>\n      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[18454, 73033, 18962, 22632, 58803]</td>\n      <td>[2017-04-04 12:19:56-07:00, 2017-04-06 10:10:5...</td>\n      <td>[5, 5, 5, 5, 5]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[37855, 19881, 45252, 83738, 78185, 9713, 9762...</td>\n      <td>[2016-03-04 06:02:24-08:00, 2016-07-23 01:02:4...</td>\n      <td>[3, 4, 3, 3, 3, 2, 1, 3, 3, 3, 4, 3]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/14973 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db7e6a83e7444504affe64ff358a95dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/14973 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8b250b18bed4fe59a4272bf0b3edbff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   user_id                                            item_id  \\\n0        0                              [55005, 61146, 47066]   \n1        1                                      [6248, 67578]   \n2        2  [44682, 85879, 50729, 4168, 14482, 78634, 7653...   \n3        3                [18454, 73033, 18962, 22632, 58803]   \n4        4  [37855, 19881, 45252, 83738, 78185, 9713, 9762...   \n\n                                            datetime  \\\n0  [2007-09-16 21:21:34-07:00, 2008-05-12 19:27:1...   \n1  [2016-05-11 04:53:09-07:00, 2016-10-07 05:08:2...   \n2  [2013-03-09 07:02:48-08:00, 2013-03-09 07:03:4...   \n3  [2017-04-04 12:19:56-07:00, 2017-04-06 10:10:5...   \n4  [2016-03-04 06:02:24-08:00, 2016-07-23 01:02:4...   \n\n                                 rating  \\\n0                             [5, 5, 0]   \n1                                [4, 5]   \n2  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]   \n3                       [5, 5, 5, 5, 5]   \n4  [3, 4, 3, 3, 3, 2, 1, 3, 3, 3, 4, 3]   \n\n                                     item_id_history        item_id_future  \\\n0                                     [55005, 61146]               [47066]   \n1                                             [6248]               [67578]   \n2  [44682, 85879, 50729, 4168, 14482, 78634, 7653...   [3651, 81594, 2856]   \n3                       [18454, 73033, 18962, 22632]               [58803]   \n4  [37855, 19881, 45252, 83738, 78185, 9713, 9762...  [83867, 57724, 4973]   \n\n                rating_history rating_future  \n0                       [5, 5]           [0]  \n1                          [4]           [5]  \n2  [5, 5, 5, 5, 5, 5, 5, 5, 5]     [5, 5, 5]  \n3                 [5, 5, 5, 5]           [5]  \n4  [3, 4, 3, 3, 3, 2, 1, 3, 3]     [3, 4, 3]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>datetime</th>\n      <th>rating</th>\n      <th>item_id_history</th>\n      <th>item_id_future</th>\n      <th>rating_history</th>\n      <th>rating_future</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[55005, 61146, 47066]</td>\n      <td>[2007-09-16 21:21:34-07:00, 2008-05-12 19:27:1...</td>\n      <td>[5, 5, 0]</td>\n      <td>[55005, 61146]</td>\n      <td>[47066]</td>\n      <td>[5, 5]</td>\n      <td>[0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[6248, 67578]</td>\n      <td>[2016-05-11 04:53:09-07:00, 2016-10-07 05:08:2...</td>\n      <td>[4, 5]</td>\n      <td>[6248]</td>\n      <td>[67578]</td>\n      <td>[4]</td>\n      <td>[5]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[44682, 85879, 50729, 4168, 14482, 78634, 7653...</td>\n      <td>[2013-03-09 07:02:48-08:00, 2013-03-09 07:03:4...</td>\n      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]</td>\n      <td>[44682, 85879, 50729, 4168, 14482, 78634, 7653...</td>\n      <td>[3651, 81594, 2856]</td>\n      <td>[5, 5, 5, 5, 5, 5, 5, 5, 5]</td>\n      <td>[5, 5, 5]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[18454, 73033, 18962, 22632, 58803]</td>\n      <td>[2017-04-04 12:19:56-07:00, 2017-04-06 10:10:5...</td>\n      <td>[5, 5, 5, 5, 5]</td>\n      <td>[18454, 73033, 18962, 22632]</td>\n      <td>[58803]</td>\n      <td>[5, 5, 5, 5]</td>\n      <td>[5]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[37855, 19881, 45252, 83738, 78185, 9713, 9762...</td>\n      <td>[2016-03-04 06:02:24-08:00, 2016-07-23 01:02:4...</td>\n      <td>[3, 4, 3, 3, 3, 2, 1, 3, 3, 3, 4, 3]</td>\n      <td>[37855, 19881, 45252, 83738, 78185, 9713, 9762...</td>\n      <td>[83867, 57724, 4973]</td>\n      <td>[3, 4, 3, 3, 3, 2, 1, 3, 3]</td>\n      <td>[3, 4, 3]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_sparse_matrix(dataframe: pd.DataFrame, item_id_column: str, value_column: str = None,\n",
    "                         shape: tuple[int, int] = None) -> sparse.csr_matrix:\n",
    "    \"\"\"\n",
    "    Creates a sparse matrix from the data in `dataframe`.\n",
    "    \"\"\"\n",
    "    # Flatten the dataframe\n",
    "    user_ids = []\n",
    "    item_ids = []\n",
    "    values = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        user_ids.extend([row[\"user_id\"]] * len(row[item_id_column]))\n",
    "        item_ids.extend(row[item_id_column])\n",
    "        if value_column is not None:\n",
    "            values.extend(row[value_column])\n",
    "    if value_column is None:\n",
    "        values = np.ones(len(user_ids))\n",
    "    # Create the CSR matrix\n",
    "    return sparse.csr_matrix((values, (user_ids, item_ids)), shape=shape, dtype=np.int32)\n",
    "\n",
    "\n",
    "shape = (processed_df[\"user_id\"].max() + 1, processed_df[\"item_id\"].max() + 1)\n",
    "train = create_sparse_matrix(sessions_df, \"item_id_history\", \"rating_history\", shape)\n",
    "true = create_sparse_matrix(sessions_df, \"item_id_future\", \"rating_future\", shape)\n",
    "# train = create_sparse_matrix(sessions_df, \"item_id_history\", None, shape)\n",
    "# true = create_sparse_matrix(sessions_df, \"item_id_future\", None, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "The evaluation metrics we will be using are the following:\n",
    "\n",
    "- Recall @ 10: the percentage of users where the top-10 recommendations are relevant.\n",
    "\n",
    "- NDCG @ 10: similar to recall but the sum of the hits is weighted by the place in the top 10.\n",
    "\n",
    "- Qualitative results, i.e. examples of the recommendations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def sparse_invert_nonzero(a: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    inverse = a.copy()\n",
    "    inverse.data = 1 / inverse.data\n",
    "    return inverse\n",
    "\n",
    "\n",
    "def sparse_divide_nonzero(a: sparse.csr_matrix, b: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    return a.multiply(sparse_invert_nonzero(b))\n",
    "\n",
    "\n",
    "def compute_recall(true: sparse.csr_matrix, predicted: sparse.csr_matrix) -> float:\n",
    "    scores = sparse.lil_matrix(predicted.shape)\n",
    "    scores[predicted.multiply(true).astype(bool)] = 1\n",
    "    scores = sparse_divide_nonzero(scores.tocsr(), sparse.csr_matrix(true.sum(axis=1))).sum(axis=1)\n",
    "    return scores.mean()\n",
    "\n",
    "# recall = compute_recall(true, predicted)\n",
    "# print(f\"Recall @ {k}: {recall:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RBM-based recommender"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class RBM:\n",
    "\n",
    "    def __init__(self, nr_visible: int, nr_hidden: int, learning_rate: float) -> None:\n",
    "        self.weights = torch.randn(nr_visible, nr_hidden).to(device)\n",
    "        self.bias_items = torch.randn(nr_visible).to(device)\n",
    "        self.bias_features = torch.randn(nr_hidden).to(device)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def prob_hidden(self, visible: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(visible @ self.weights + self.bias_features)\n",
    "\n",
    "    def prob_visible(self, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(hidden @ self.weights.t() + self.bias_items)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(probabilities: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.bernoulli(probabilities)\n",
    "\n",
    "    def update(self, visible_0: torch.Tensor, visible_k: torch.Tensor, prob_hidden_0: torch.Tensor,\n",
    "               prob_hidden_k: torch.Tensor) -> None:\n",
    "        self.weights += self.learning_rate * (visible_0.t() @ prob_hidden_0 - visible_k.t() @ prob_hidden_k)\n",
    "        self.bias_items += self.learning_rate * (visible_0 - visible_k).sum(0)\n",
    "        self.bias_features += self.learning_rate * (prob_hidden_0 - prob_hidden_k).sum(0)\n",
    "\n",
    "    def fit(self, visible_0: torch.Tensor, sampling_iterations: int = 1) -> torch.Tensor:\n",
    "        visible_k = visible_0\n",
    "        for _ in range(sampling_iterations):\n",
    "            hidden_k = self.sample(self.prob_hidden(visible_k))\n",
    "            visible_k = self.sample(self.prob_visible(hidden_k))\n",
    "            visible_k[visible_0 < 0] = visible_0[visible_0 < 0]\n",
    "        prob_hidden_0 = self.sample(self.prob_hidden(visible_0))\n",
    "        prob_hidden_k = self.sample(self.prob_hidden(visible_k))\n",
    "        self.update(visible_0, visible_k, prob_hidden_0, prob_hidden_k)\n",
    "        return visible_k\n",
    "\n",
    "    def predict(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        hidden = self.sample(self.prob_hidden(data))\n",
    "        return self.prob_visible(hidden)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert(data: sparse.csr_matrix) -> torch.Tensor:\n",
    "        tensor = torch.Tensor(data.toarray()).to(device)\n",
    "        tensor[tensor == 0] = -1\n",
    "        tensor[tensor == 1] = 0\n",
    "        tensor[tensor == 2] = 0\n",
    "        tensor[tensor >= 3] = 1\n",
    "        return tensor\n",
    "\n",
    "\n",
    "# rbm = RBM(train.shape[1], 50, 0.1)\n",
    "#\n",
    "# epochs = 10  # TODO: remove\n",
    "# batch_size = 100\n",
    "# for epoch in tqdm(range(epochs), leave=False):\n",
    "#     train_loss = 0\n",
    "#     for batch_start in tqdm(range(0, train.shape[0], batch_size), leave=False):\n",
    "#         batch = rbm.convert(train[batch_start : batch_start + batch_size])\n",
    "#         reconstruction = rbm.fit(batch)\n",
    "#         train_loss += torch.abs(batch[batch >= 0] - reconstruction[batch >= 0]).sum()\n",
    "#     tqdm.write(f\"\\rEpoch {epoch + 1:<2}: {(train_loss / train.shape[0]).item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class RBMRecommender:\n",
    "    \"\"\"\n",
    "    A recommender system for ratings based on Restricted Boltzmann Machines (RBMs).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nr_items: int, nr_ratings: int, nr_hidden: int, learning_rate: float, k: int = 10) -> None:\n",
    "        # Training and evaluation parameters\n",
    "        self.nr_items = nr_items\n",
    "        self.nr_ratings = nr_ratings\n",
    "        self.nr_hidden = nr_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.k = k\n",
    "        # RBM weights and biases\n",
    "        self.weights = torch.randn(nr_ratings, nr_items, nr_hidden).to(device)\n",
    "        self.bias_items = torch.randn(nr_ratings, nr_items).to(device)\n",
    "        self.bias_features = torch.randn(nr_hidden).to(device)\n",
    "\n",
    "    def probability_hidden(self, visible: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Computes `p(h_j = 1 | V)`. \"\"\"\n",
    "        # Using negative indices so that both batched and single data vectors work\n",
    "        temp = visible.unsqueeze(-2) @ self.weights  # sum_i(v_i^k * W_{ij}^k)\n",
    "        temp = temp.sum(-3)  # sum_k(sum_i(v_i^k * W_{ij}^k))\n",
    "        return torch.sigmoid(temp + self.bias_features)\n",
    "\n",
    "    def probability_visible(self, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Computes `p(v^k_i = 1 | h)`. \"\"\"\n",
    "        temp = self.weights @ hidden.unsqueeze(-1)  # sum_j(h_j * W_{ij}^k)\n",
    "        return torch.softmax(temp.squeeze() + self.bias_items, 0)\n",
    "\n",
    "    def sample_hidden(self, visible: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Samples randomly from the distribution defined by the probabilities given by probability_hidden(). \"\"\"\n",
    "        return torch.bernoulli(self.probability_hidden(visible))\n",
    "\n",
    "    def sample_visible(self, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Samples randomly from the distribution defined by the probabilities given by probability_visible(). \"\"\"\n",
    "        probabilities = self.probability_visible(hidden).transpose(1, 2)\n",
    "        distribution = torch.distributions.Multinomial(1, probs=probabilities)\n",
    "        return distribution.sample().transpose(1, 2)\n",
    "\n",
    "    def gibbs_sampling(self, data: torch.Tensor, steps: int = 1) -> torch.Tensor:\n",
    "        \"\"\" Runs a Gibbs sampler for some number of steps. \"\"\"\n",
    "        visible = data\n",
    "        for _ in range(steps):\n",
    "            hidden = self.sample_hidden(visible)\n",
    "            visible = self.sample_visible(hidden)\n",
    "            visible[data == 0] = 0\n",
    "        return visible\n",
    "\n",
    "    def fit(self, data: sparse.csr_matrix, batch_size: int = 1, sampling_iterations: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Trains the underlying RBM with the given input data. For now, we will ignore the value of the ratings, and\n",
    "        instead look at the data as a binary interaction matrix (i.e. a value => 1, no value => 0).\n",
    "\n",
    "        :param data: A |users|x|items| sparse matrix, where each entry is the rating given by that user to that item.\n",
    "        :param batch_size: The number of users to use per mini-batch.\n",
    "        :param sampling_iterations: The number of iterations of alternating Gibbs sampling per iteration of CD.\n",
    "        \"\"\"\n",
    "        for batch_start in tqdm(range(0, data.shape[0], batch_size)):\n",
    "            if batch_size == 1:\n",
    "                batch = self.convert_input(data[batch_start])\n",
    "            else:\n",
    "                batch = self.convert_batch(data[batch_start: batch_start + batch_size], batch_size)\n",
    "\n",
    "            visible = self.gibbs_sampling(batch, sampling_iterations)\n",
    "\n",
    "            prob_hidden_original = self.probability_hidden(batch)\n",
    "            prob_hidden_final = self.probability_hidden(visible)\n",
    "            data_sample = batch.unsqueeze(2) @ prob_hidden_original\n",
    "            reconstruction_sample = visible.unsqueeze(2) @ prob_hidden_final\n",
    "\n",
    "            self.weights += self.learning_rate * (data_sample - reconstruction_sample) / data.shape[0]\n",
    "            self.bias_items += self.learning_rate * (batch - visible).sum(0) / data.shape[0]\n",
    "            self.bias_features += self.learning_rate * (prob_hidden_original - prob_hidden_final).sum(0) / data.shape[0]\n",
    "\n",
    "    def predict(self, data: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "        \"\"\" Returns predictions for each user in the data matrix. \"\"\"\n",
    "        matrix_data = ([], ([], []))  # (scores, (user IDs, item IDs))\n",
    "\n",
    "        for user_id, history in enumerate(data):\n",
    "            hidden = self.sample_hidden(torch.Tensor(history.toarray()))\n",
    "            visible = self.probability_visible(hidden).flatten()\n",
    "            item_ids = np.argpartition(-visible, self.k)[: self.k]\n",
    "            scores = [visible[item_id] for item_id in item_ids]\n",
    "\n",
    "            matrix_data[0].extend(scores)\n",
    "            matrix_data[1][0].extend([user_id] * self.k)\n",
    "            matrix_data[1][1].extend(item_ids)\n",
    "        return sparse.csr_matrix(matrix_data)\n",
    "\n",
    "    def convert_input(self, data: sparse.csr_matrix) -> torch.Tensor:\n",
    "        \"\"\" Converts a single user's rating vector into a one-hot-encoded matrix. \"\"\"\n",
    "        matrix = torch.zeros(self.nr_ratings, self.nr_items).to(device)\n",
    "        matrix[data.data, data.indices] = 1\n",
    "        return matrix\n",
    "\n",
    "    def convert_batch(self, data: sparse.csr_matrix, batch_size: int = None) -> torch.Tensor:\n",
    "        \"\"\" Converts a batch of ratings into a one-hot-encoded matrix. \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = data.shape[0]\n",
    "        matrix = torch.zeros(batch_size, self.nr_ratings, self.nr_items).to(device)\n",
    "        data = data.tocoo(copy=False)\n",
    "        matrix[data.row, data.data, data.col] = 1\n",
    "        return matrix\n",
    "\n",
    "\n",
    "recommender = RBMRecommender(train.shape[1], 6, 100, 0.1, k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 89407])\n",
      "tensor([[[0., 1., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 1., 1.,  ..., 0., 1., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 1.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "         [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "data = recommender.convert_batch(train[: 10], 10)\n",
    "hidden = recommender.sample_hidden(data)\n",
    "visible = recommender.sample_visible(hidden)\n",
    "\n",
    "print(visible.shape)\n",
    "print(visible)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "#\n",
    "# recommender = RBMRecommender(train.shape[1], 6, 100, 0.1, k)\n",
    "# for _ in range(epochs):\n",
    "#     recommender.fit(train, batch_size=1)\n",
    "#     break\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for user_id in range(train.shape[0]):\n",
    "#     if user_id == 10:\n",
    "#         break\n",
    "#     elif train.getrow(user_id).getnnz() == 0:\n",
    "#         continue\n",
    "#\n",
    "#     prediction = recommender.predict(train.getrow(user_id))\n",
    "#     print(\"User\", user_id)\n",
    "#     print(\"> train\", train.getrow(user_id), sep=\"\\n\")\n",
    "#     print(\"> true\", true.getrow(user_id), sep=\"\\n\")\n",
    "#     print(\"> prediction\", prediction, sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}