{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Following the tutorial and implementation found here:\n",
    "https://developer.ibm.com/tutorials/build-a-recommendation-engine-with-a-restricted-boltzmann-machine-using-tensorflow/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: numpy==1.21.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.21.3)\r\n",
      "Requirement already satisfied: matplotlib==3.4.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.4.3)\r\n",
      "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.3.4)\r\n",
      "Requirement already satisfied: scikit-learn==1.0.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.0.1)\r\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (4.62.3)\r\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.5)\r\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.2.0)\r\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (7.6.5)\r\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (1.3.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (0.11.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.8.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (8.4.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.4->-r requirements.txt (line 4)) (2021.3)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (1.7.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (1.1.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (3.0.0)\r\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.41.1)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.0)\r\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.14.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/lib/python3/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.34.2)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.5.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.2)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.19.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.0)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (12.0.0)\r\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.10.0.2)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.6.3)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.15.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.21.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.13.3)\r\n",
      "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (22.3.0)\r\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (4.9.1)\r\n",
      "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (7.0.6)\r\n",
      "Requirement already satisfied: traitlets in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (2.10.0)\r\n",
      "Requirement already satisfied: qtpy in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (1.11.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.2)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.1)\r\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.0)\r\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (6.1)\r\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (4.4.0)\r\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (21.1.0)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.2)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.4)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.3)\r\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (4.1.0)\r\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.4)\r\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.0)\r\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.0)\r\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (7.29.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (3.5.2)\r\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (1.0.2)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.22)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.6)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.4)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0.2)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (58.5.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.8.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.3.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.22.0)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.0.1)\r\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (4.2.0)\r\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.15.0)\r\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (21.2)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.17.2)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.3)\r\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (4.8.0)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.5)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.5)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.7.2)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.2.4)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.18.0)\r\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (5.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (21.2.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.20)\r\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.1.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.8)\r\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources; python_version < \"3.9\"->jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (3.6.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc6a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from IPython.display import display\n",
    "\n",
    "from json import loads\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Generator, Iterable, Union, TypeVar\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597fd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(filename: str, read_max: int = None, attributes: Iterable[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the file line by line, parsing each line as json.\n",
    "\n",
    "    :param filename: The path to the datafile.\n",
    "    :param read_max: The maximum number of lines to read from the datafile.\n",
    "    :param attributes: The attributes of each JSON object that should be extracted; other attributes are ignored.\n",
    "    \"\"\"\n",
    "    file = gzip.open(filename, \"r\")\n",
    "    data = []\n",
    "    for index, line in enumerate(tqdm(file)):\n",
    "        if index == read_max:\n",
    "            break\n",
    "        entry = loads(line)\n",
    "        if attributes is not None:\n",
    "            entry = {key: entry[key] for key in attributes}\n",
    "        data.append(entry)\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a8f84d9202c4fe08efee08e331e62a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "069f95880da24aaaa17b6c80417f9d6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    book_id                                              title\n0  25742454                              The Switchblade Mamma\n1  30128855                                            Cruelle\n2  13571772  Captain America: Winter Soldier (The Ultimate ...\n3  35452242  Bounty Hunter 4/3: My Life in Combat from Mari...\n4    707611                          Superman Archives, Vol. 2\n5   2250580                            A.I. Revolution, Vol. 1\n6  27036536                              War Stories, Volume 3\n7  27036537                                 Crossed, Volume 15\n8  27036538  Crossed + One Hundred, Volume 2 (Crossed +100 #2)\n9  27036539                              War Stories, Volume 4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25742454</td>\n      <td>The Switchblade Mamma</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30128855</td>\n      <td>Cruelle</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13571772</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35452242</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>707611</td>\n      <td>Superman Archives, Vol. 2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2250580</td>\n      <td>A.I. Revolution, Vol. 1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>27036536</td>\n      <td>War Stories, Volume 3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27036537</td>\n      <td>Crossed, Volume 15</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27036538</td>\n      <td>Crossed + One Hundred, Volume 2 (Crossed +100 #2)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27036539</td>\n      <td>War Stories, Volume 4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d    836610       0\n1  8842281e1d1347389f2ab93d60773d4d   7648967       0\n2  8842281e1d1347389f2ab93d60773d4d  15704307       0\n3  8842281e1d1347389f2ab93d60773d4d   6902644       0\n4  8842281e1d1347389f2ab93d60773d4d   9844623       0\n5  8842281e1d1347389f2ab93d60773d4d  13163846       0\n6  8842281e1d1347389f2ab93d60773d4d   1137635       0\n7  8842281e1d1347389f2ab93d60773d4d     44735       0\n8  8842281e1d1347389f2ab93d60773d4d    472331       0\n9  8842281e1d1347389f2ab93d60773d4d     24815       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>13163846</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1137635</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>44735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>472331</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24815</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"data/\"\n",
    "books = f\"{data_path}goodreads_books_comics_graphic.json.gz\"\n",
    "interactions = f\"{data_path}goodreads_interactions_comics_graphic.json.gz\"\n",
    "reviews = f\"{data_path}goodreads_reviews_comics_graphic.json.gz\"\n",
    "\n",
    "n = None\n",
    "\n",
    "books_df = parse_json(books, n, (\"book_id\", \"title\"))\n",
    "interactions_df = parse_json(interactions, n, (\"user_id\", \"book_id\", \"rating\"))\n",
    "\n",
    "display(books_df.head(10))\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the columns to the correct types\n",
    "books_df[\"book_id\"] = books_df[\"book_id\"].astype(\"int64\")\n",
    "interactions_df[\"book_id\"] = interactions_df[\"book_id\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d    836610       0\n1  8842281e1d1347389f2ab93d60773d4d   7648967       0\n2  8842281e1d1347389f2ab93d60773d4d  15704307       0\n3  8842281e1d1347389f2ab93d60773d4d   6902644       0\n4  8842281e1d1347389f2ab93d60773d4d   9844623       0\n5  8842281e1d1347389f2ab93d60773d4d  13163846       0\n6  8842281e1d1347389f2ab93d60773d4d   1137635       0\n7  8842281e1d1347389f2ab93d60773d4d     44735       0\n8  8842281e1d1347389f2ab93d60773d4d    472331       0\n9  8842281e1d1347389f2ab93d60773d4d     24815       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>13163846</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1137635</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>44735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>472331</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24815</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 342415\n",
      "Number of unique items: 89411\n",
      "7347630 initial rows\n",
      "6995891 rows after preprocessing\n",
      "Number of unique users: 148438\n",
      "Number of unique items: 89276\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d    836610       0\n1  8842281e1d1347389f2ab93d60773d4d   7648967       0\n2  8842281e1d1347389f2ab93d60773d4d  15704307       0\n3  8842281e1d1347389f2ab93d60773d4d   6902644       0\n4  8842281e1d1347389f2ab93d60773d4d   9844623       0\n5  8842281e1d1347389f2ab93d60773d4d  13163846       0\n6  8842281e1d1347389f2ab93d60773d4d   1137635       0\n7  8842281e1d1347389f2ab93d60773d4d     44735       0\n8  8842281e1d1347389f2ab93d60773d4d    472331       0\n9  8842281e1d1347389f2ab93d60773d4d     24815       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>13163846</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1137635</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>44735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>472331</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24815</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(dataframe: pd.DataFrame, min_support: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes users with fewer than `min_support` interactions, and duplicate user-item pairs (which do not exist in the\n",
    "    dataset anyway). Items with very few interactions are not removed, unless they have no interactions at all after\n",
    "    removing infrequent users.\n",
    "    \"\"\"\n",
    "    print(dataframe.shape[0], \"initial rows\")\n",
    "    # Drop reconsumption items\n",
    "    dataframe = dataframe.drop_duplicates(subset=[\"user_id\", \"book_id\"])\n",
    "    # Drop users with less than `min_support` interactions\n",
    "    items_per_user = dataframe.groupby(\"user_id\", as_index=False)[\"book_id\"].size()\n",
    "    items_per_user = items_per_user.rename({\"size\": \"items_per_user\"}, axis=\"columns\")\n",
    "    dataframe = pd.merge(dataframe, items_per_user, how=\"left\", on=[\"user_id\"])\n",
    "    dataframe = dataframe[dataframe[\"items_per_user\"] >= min_support]\n",
    "    # Report and clean up after the preprocessing\n",
    "    print(dataframe.shape[0], \"rows after preprocessing\")\n",
    "    dataframe.drop(columns=[\"items_per_user\"], inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "display(interactions_df.head(10))\n",
    "print(f\"Number of unique users:\", interactions_df[\"user_id\"].nunique())\n",
    "print(f\"Number of unique items:\", interactions_df[\"book_id\"].nunique())\n",
    "interactions_df = preprocess(interactions_df, min_support=5)\n",
    "print(f\"Number of unique users:\", interactions_df[\"user_id\"].nunique())\n",
    "print(f\"Number of unique items:\", interactions_df[\"book_id\"].nunique())\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6995891 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49685b78241f492e821ea5931bd4ab24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6995891 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68df5d0d113742f8a8d00893b57b0a1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/89411 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04b123954b024411a2ad218a169e705b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    book_id                                              title  book_id_int\n0  25742454                              The Switchblade Mamma        89095\n1  30128855                                            Cruelle        51852\n2  13571772  Captain America: Winter Soldier (The Ultimate ...        41242\n3  35452242  Bounty Hunter 4/3: My Life in Combat from Mari...        88960\n4    707611                          Superman Archives, Vol. 2        54601\n5   2250580                            A.I. Revolution, Vol. 1        28661\n6  27036536                              War Stories, Volume 3        11531\n7  27036537                                 Crossed, Volume 15        11422\n8  27036538  Crossed + One Hundred, Volume 2 (Crossed +100 #2)        11373\n9  27036539                              War Stories, Volume 4        10885",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>title</th>\n      <th>book_id_int</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25742454</td>\n      <td>The Switchblade Mamma</td>\n      <td>89095</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30128855</td>\n      <td>Cruelle</td>\n      <td>51852</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13571772</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n      <td>41242</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35452242</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n      <td>88960</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>707611</td>\n      <td>Superman Archives, Vol. 2</td>\n      <td>54601</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2250580</td>\n      <td>A.I. Revolution, Vol. 1</td>\n      <td>28661</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>27036536</td>\n      <td>War Stories, Volume 3</td>\n      <td>11531</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27036537</td>\n      <td>Crossed, Volume 15</td>\n      <td>11422</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27036538</td>\n      <td>Crossed + One Hundred, Volume 2 (Crossed +100 #2)</td>\n      <td>11373</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27036539</td>\n      <td>War Stories, Volume 4</td>\n      <td>10885</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   user_id  item_id  rating\n0        0        0       0\n1        0        1       0\n2        0        2       0\n3        0        3       0\n4        0        4       0\n5        0        5       0\n6        0        6       0\n7        0        7       0\n8        0        8       0\n9        0        9       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>9</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_consecutive_mapping(dataframe: pd.DataFrame, column: str, new_column: str, *additional: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generates a consecutive ID column for the values of an existing column. Also adds this column to additional data\n",
    "    frames with the exact same mapping of old ID to new (consecutive) ID.\n",
    "    \"\"\"\n",
    "    ids = {}\n",
    "\n",
    "    def map_to_consecutive_ids(uuid: Union[int, np.int64]) -> int:\n",
    "        \"\"\"\n",
    "        To be used with `pd.Dataframe.apply()` or `pd.Dataframe.progress_apply()`; returns a unique ID per distinct\n",
    "        value.\n",
    "        \"\"\"\n",
    "        if uuid not in ids:\n",
    "            ids[uuid] = len(ids)\n",
    "        return ids[uuid]\n",
    "\n",
    "    dataframe[new_column] = dataframe[column].progress_apply(map_to_consecutive_ids)\n",
    "    for frame in additional:\n",
    "        frame[new_column] = frame[column].progress_apply(lambda old_id: ids.get(old_id, -1))\n",
    "\n",
    "\n",
    "apply_consecutive_mapping(interactions_df, \"user_id\", \"user_id_int\")\n",
    "apply_consecutive_mapping(interactions_df, \"book_id\", \"book_id_int\", books_df)\n",
    "\n",
    "interactions_df = interactions_df[[\"user_id_int\", \"book_id_int\", \"rating\"]]\n",
    "interactions_df = interactions_df.rename(columns={\"user_id_int\": \"user_id\", \"book_id_int\": \"item_id\"})\n",
    "\n",
    "display(books_df.head(10))\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataType = TypeVar(\"DataType\", pd.DataFrame, sparse.csr_matrix)\n",
    "\n",
    "\n",
    "def generate_random_split(data: DataType, seed: int = None) -> tuple:\n",
    "    return train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "def split_k_folds(data: sparse.csr_matrix, nr_folds: int = 5) -> Generator[tuple, None, None]:\n",
    "    \"\"\"\n",
    "    Generates K train-test splits for K-fold cross validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nr_folds)\n",
    "    yield from kf.split(data)\n",
    "\n",
    "\n",
    "def create_sparse_matrix(dataframe: pd.DataFrame, shape: tuple = None) -> sparse.csr_matrix:\n",
    "    return sparse.csr_matrix((dataframe[\"rating\"], (dataframe[\"user_id\"], dataframe[\"item_id\"])), shape=shape)\n",
    "\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X) -> tf.SparseTensor:\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, np.float32(coo.data), coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shape = (interactions_df[\"user_id\"].max() + 1, interactions_df[\"item_id\"].max() + 1)\n",
    "interactions_df[\"rating\"] = interactions_df[\"rating\"].apply(lambda value: (value + 1) / 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We then move on to creating the visible and hidden layer units and setting their activation functions. In this case, we\n",
    "will be using the <code>tf.sigmoid</code> and <code>tf.relu</code> functions as nonlinear activations since it is\n",
    "commonly used in RBM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input processing: Defining a function to return only the generated hidden states\n",
    "\n",
    "def hidden_layer(v0_state, W, hb):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + hb)\n",
    "    return tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))\n",
    "\n",
    "\n",
    "def reconstructed_output(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)\n",
    "    return tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))\n",
    "\n",
    "\n",
    "def gibbs_sampling(v0_state: tf.Tensor, weights: tf.Variable, bias_visible: tf.Variable, bias_hidden: tf.Variable,\n",
    "                   nr_iterations: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs gibbs sampling starting with the given input vector.\n",
    "    :return: A tuple consisting of (in order):\n",
    "        - The hidden layer `h0_state`, sampled from the given data vector `v0_state`.\n",
    "        - The final visible layer `vk_state`.\n",
    "        - The final hidden layer `hk_state`.\n",
    "    \"\"\"\n",
    "    h0_state = hidden_layer(v0_state, weights, hb)\n",
    "    vk_state = v0_state\n",
    "    hk_state = h0_state\n",
    "    for _ in range(nr_iterations):\n",
    "        vk_state = reconstructed_output(hk_state, weights, bias_visible)\n",
    "        hk_state = hidden_layer(vk_state, weights, bias_hidden)\n",
    "    return h0_state, vk_state, hk_state\n",
    "\n",
    "def get_k_recommendations(user_input: tf.Tensor, W: tf.Tensor, hb: tf.Tensor, vb: tf.Tensor,\n",
    "                          k: int) -> sparse.csr_matrix:\n",
    "    user_input = tf.convert_to_tensor(user_input, \"float32\")\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(user_input, W) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    scores = vv1.numpy()\n",
    "    scores[user_input.numpy() > 0] = 0\n",
    "    recommendations = np.argpartition(-scores, k, 1)[:, :k]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conditional RBM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def hidden_layer_conditional(v0_state, W, hb, r, D):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + tf.matmul(r, D) + hb)\n",
    "    return tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))\n",
    "\n",
    "\n",
    "def reconstructed_output_conditional(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)\n",
    "    return tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))\n",
    "\n",
    "\n",
    "def gibbs_sampling_conditional(v0_state: tf.Tensor, weights: tf.Variable, r: tf.Tensor, cond_weights: tf.Variable,\n",
    "                   bias_visible: tf.Variable, bias_hidden: tf.Variable, nr_iterations: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs gibbs sampling starting with the given input vector.\n",
    "    :return: A tuple consisting of (in order):\n",
    "        - The hidden layer `h0_state`, sampled from the given data vector `v0_state`.\n",
    "        - The final visible layer `vk_state`.\n",
    "        - The final hidden layer `hk_state`.\n",
    "    \"\"\"\n",
    "    h0_state = hidden_layer_conditional(v0_state, weights, hb, r, cond_weights)\n",
    "    vk_state = v0_state\n",
    "    hk_state = h0_state\n",
    "    for _ in range(nr_iterations):\n",
    "        vk_state = reconstructed_output_conditional(hk_state, weights, bias_visible)\n",
    "        hk_state = hidden_layer_conditional(vk_state, weights, bias_hidden, r, cond_weights)\n",
    "    return h0_state, vk_state, hk_state\n",
    "\n",
    "def get_k_recommendations_conditional(user_input: tf.Tensor, r: tf.Tensor, W: tf.Tensor, D: tf.Tensor, hb: tf.Tensor,\n",
    "                                      vb: tf.Tensor, k: int) -> sparse.csr_matrix:\n",
    "    user_input = tf.convert_to_tensor(user_input, \"float32\")\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(user_input, W) + tf.matmul(r, D) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    scores = vv1.numpy()\n",
    "    scores[user_input.numpy() > 0] = 0\n",
    "    recommendations = np.argpartition(-scores, k, 1)[:, :k]\n",
    "    return recommendations\n",
    "\n",
    "def get_r(data: tf.SparseTensor) -> tf.Tensor:\n",
    "    values = np.ones(len(data.indices))\n",
    "    indices = data.indices.numpy().T\n",
    "    sparse_r = sparse.coo_matrix((values, indices), shape=data.shape)\n",
    "    return tf.constant(sparse_r.toarray(), tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error(v0_state, v1_state):\n",
    "    \"\"\"\n",
    "    Returns the sum of the squared reconstruction errors. This error is computed per batch, and should be accumulated\n",
    "    per epoch. At the end of the epoch the total RMSE can then be computed from that sum.\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(tf.reduce_mean(tf.square(v0_state - v1_state), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def array_to_sparse_matrix(recommendations: np.ndarray) -> sparse.csr_matrix:\n",
    "    user_ids = np.repeat(np.arange(shape[0]), recommendations.shape[1])\n",
    "    item_ids = recommendations.flatten()\n",
    "    scores = np.ones(shape[0] * recommendations.shape[1])\n",
    "    return sparse.csr_matrix((scores, (user_ids, item_ids)), shape=shape)\n",
    "\n",
    "\n",
    "def sparse_invert_nonzero(a: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    inverse = a.copy()\n",
    "    inverse.data = 1 / inverse.data\n",
    "    return inverse\n",
    "\n",
    "\n",
    "def sparse_divide_nonzero(a: sparse.csr_matrix, b: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    return a.multiply(sparse_invert_nonzero(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def compute_recall(true: sparse.csr_matrix, predicted: sparse.csr_matrix) -> float:\n",
    "    scores = sparse.lil_matrix(predicted.shape)\n",
    "    scores[predicted.multiply(true).astype(bool)] = 1\n",
    "    scores = sparse_divide_nonzero(scores.tocsr(), sparse.csr_matrix(true.sum(axis=1))).sum(axis=1)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def get_ndcg(top_k: np.ndarray, actual_scores: np.ndarray):\n",
    "    numerator = actual_scores[top_k]\n",
    "    denom = np.log2(np.arange(2, len(top_k) + 2))\n",
    "    # Calculate DCG and IDCG\n",
    "    dcg = np.divide(numerator, denom).sum()\n",
    "    if dcg == 0:\n",
    "        return 0\n",
    "    # Sort the scores based on relevance\n",
    "    ideal_numerator = np.sort(numerator)[::-1]\n",
    "    idcg = np.divide(ideal_numerator, denom).sum()\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "def compute_ndcg(true: sparse.csr_matrix, recommendations: np.ndarray) -> float:\n",
    "    ndcg_sum = 0\n",
    "    for user_id in trange(true.shape[0], leave=False):\n",
    "        ndcg_sum += get_ndcg(recommendations[user_id], true[user_id].toarray()[0])\n",
    "    return ndcg_sum / true.shape[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_rbm(train: tf.data.Dataset, validation: tf.data.Dataset, weights: tf.Variable, vb: tf.Variable,\n",
    "              hb: tf.Variable, cond_weights: tf.Variable = None) -> tuple:\n",
    "    \"\"\"\n",
    "    The training loop of the (conditional) RBM.\n",
    "\n",
    "    :param train: The training dataset.\n",
    "    :param validation: The validation dataset.\n",
    "    :param weights: The weights of the RBM.\n",
    "    :param vb: The biases of the visible layer of the RBM.\n",
    "    :param hb: The biases of the hidden layer of the RBM.\n",
    "    :param cond_weights: The interaction vector weights of the conditional RBM.\n",
    "\n",
    "    :return: A tuple containing the final weights, biases, training errors (RMSE), and validation errors.\n",
    "    \"\"\"\n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "\n",
    "    for _ in trange(epochs, leave=False):\n",
    "        train_errors.append(0)\n",
    "        train_iter = iter(train)\n",
    "        for _ in trange(len(train), leave=False):\n",
    "            batch = next(train_iter)\n",
    "            v0_state = tf.sparse.to_dense(batch)\n",
    "\n",
    "            if cond_weights is None:\n",
    "                h0_state, vk_state, hk_state = gibbs_sampling(v0_state, weights, vb, hb, gibbs_sampling_iterations)\n",
    "            else:\n",
    "                r = get_r(batch)\n",
    "                h0_state, vk_state, hk_state = gibbs_sampling_conditional(\n",
    "                    v0_state, weights, r, cond_weights, vb, hb, gibbs_sampling_iterations)\n",
    "\n",
    "            temp_0 = tf.matmul(tf.transpose(v0_state), tf.squeeze(h0_state))\n",
    "            temp_k = tf.matmul(tf.transpose(vk_state), tf.squeeze(hk_state))\n",
    "            delta = (temp_0 - temp_k) / v0_state.shape[0]\n",
    "\n",
    "            weights = weights + alpha * delta\n",
    "            vb = vb + alpha * tf.reduce_mean(v0_state - vk_state, 0)\n",
    "            hb = hb + alpha * tf.reduce_mean(tf.squeeze(h0_state - hk_state), 0)\n",
    "            if cond_weights is not None:\n",
    "                cond_weights = cond_weights + alpha * tf.matmul(tf.transpose(r), h0_state - hk_state)\n",
    "\n",
    "            train_errors[-1] += error(v0_state, vk_state)\n",
    "\n",
    "        validation_errors.append(0)\n",
    "        validation_iter = iter(validation)\n",
    "        for _ in trange(len(validation), leave=False):\n",
    "            batch = next(validation_iter)\n",
    "            v0_state = tf.sparse.to_dense(batch)\n",
    "            if cond_weights is None:\n",
    "                _, vk_state, _ = gibbs_sampling(v0_state, weights, vb, hb, 1)\n",
    "            else:\n",
    "                _, vk_state, _ = gibbs_sampling_conditional(v0_state, weights, get_r(batch), cond_weights, vb, hb, 1)\n",
    "            validation_errors[-1] += error(v0_state, vk_state)\n",
    "\n",
    "        train_errors[-1] = (train_errors[-1] / len(train)) ** 0.5\n",
    "        validation_errors[-1] = (validation_errors[-1] / len(validation)) ** 0.5\n",
    "        # print(f\"Epoch {epoch + 1:3}:  {train_errors[-1]:10.5}  ;  {validation_errors[-1]:10.5}\")\n",
    "\n",
    "    return weights, cond_weights, vb, hb, train_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_errors(train_errors: list, validation_errors: list, file_name: str = None) -> None:\n",
    "    plt.plot(range(1, epochs + 1), train_errors, label=\"Train\")\n",
    "    plt.plot(range(1, epochs + 1), validation_errors, label=\"Validation\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    if file_name is not None:\n",
    "        plt.savefig(\"rmse.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "gibbs_sampling_iterations = 1\n",
    "nr_recommendations = 10\n",
    "alpha = 0.01  # Learning rate\n",
    "\n",
    "hiddenUnits = 1000\n",
    "visibleUnits = shape[1]\n",
    "\n",
    "seed = 1\n",
    "rng = tf.random.Generator.from_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2f4a53d70bb44f0bc932b1e00f173b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a37d351f7b5649e88ddc92e2865a8324"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/297 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "571687a67b4e4ea7aee9239e95f48a2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1485 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8b2613b784544249666e4b301f9be53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10 = 0.1591167916580949\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/148438 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d69ee949db446b90e260ee1c9490d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @ 10 = 0.12034707569009573\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7db061858c734b2186226aead12d5fb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb7c1812e55545788f24f10c3b6debc6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/297 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a87bc60f27d74d85b425ffa8fec24dd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1485 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c821279b8d524acaaa733467129cc3cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10 = 0.1526665578422727\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/148438 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef866093fea64a4b9776d0437f5ab046"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @ 10 = 0.11746154349998851\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32c6123473b1448eb4ef5a6d17abf38f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55c9d7bae77e48a8b352efa23aa4071b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/297 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6045d738cd424094ad249f9f1372e1a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1485 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73cbc4ca3b3947e0b05f408e17cba306"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10 = 0.1558141246519532\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/148438 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4533a958d40f48cfb2f3b626864a9b27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @ 10 = 0.11999903001292443\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3999f1a37534b04b2fa67d09ea49bee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85d8cbee405847659aa45935256d0aad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/297 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e34055e322f45758bd8d029d7ba2c9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1485 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce692e8f9ed94cabb1a3f2b521a2cc01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10 = 0.1576161949038565\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/148438 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea1c0cb8c5c64f079687e82ba03121c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @ 10 = 0.11896131874788003\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "436071a27d204b83a32403e9ca6f734c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51654aef6a3f4bb9a534eee8726558d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/297 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba59920186084931beb7b65dfe3b6007"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1485 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "deae86852b4649b284884ec50eb27431"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10 = 0.17424194452921904\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/148438 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4568cc563fa648989ed927e5c13e8742"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG @ 10 = 0.1262596255013874\n"
     ]
    }
   ],
   "source": [
    "W, D, vb, hb = None, None, None, None\n",
    "\n",
    "train_df, test_df = generate_random_split(interactions_df, seed=seed)\n",
    "train = create_sparse_matrix(train_df, shape)\n",
    "test = create_sparse_matrix(test_df, shape)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for train_indices, validation_indices in split_k_folds(train):\n",
    "    tensor_train = convert_sparse_matrix_to_sparse_tensor(train[train_indices])\n",
    "    tensor_validation = convert_sparse_matrix_to_sparse_tensor(train[validation_indices])\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(tensor_train).batch(batch_size)\n",
    "    validation_ds = tf.data.Dataset.from_tensor_slices(tensor_validation).batch(batch_size)\n",
    "\n",
    "    W = tf.Variable(rng.normal([visibleUnits, hiddenUnits], 0, 0.1), tf.float32)\n",
    "    # D = tf.Variable(rng.normal([visibleUnits, hiddenUnits], 0, 0.1), tf.float32)\n",
    "    D = None\n",
    "    vb = tf.Variable(rng.normal([visibleUnits], 0, 0.1), tf.float32)\n",
    "    hb = tf.Variable(rng.normal([hiddenUnits], 0, 0.1), tf.float32)\n",
    "\n",
    "    W, D, vb, hb, train_errors, validation_errors = train_rbm(train_ds, validation_ds, W, vb, hb, D)\n",
    "\n",
    "    recommendations = np.empty((shape[0], 10), dtype=np.int_)\n",
    "    for batch_start in trange(0, train.shape[0], batch_size, leave=False):\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch = train[batch_start: batch_end]\n",
    "        if D is None:\n",
    "            recommendations[batch_start: batch_end] = get_k_recommendations(\n",
    "                    batch.toarray(), W, hb, vb, nr_recommendations)\n",
    "        else:\n",
    "            recommendations[batch_start: batch_end] = get_k_recommendations_conditional(\n",
    "                    batch.toarray(), get_r(batch), W, D, hb, vb, nr_recommendations)\n",
    "\n",
    "    recall = compute_recall(test, array_to_sparse_matrix(recommendations))\n",
    "    print(\"Recall @\", nr_recommendations, \"=\", recall)\n",
    "\n",
    "    ndcg = compute_ndcg(test, recommendations)\n",
    "    print(\"NDCG @\", nr_recommendations, \"=\", ndcg)\n",
    "\n",
    "    metrics.append({\n",
    "        \"recall\": recall,\n",
    "        \"ndcg\": ndcg,\n",
    "        \"train\": train_errors,\n",
    "        \"validation\": validation_errors,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recall @ 10 = 0.15989 ± 0.0074905\n",
      "Mean NDCG @ 10 = 0.12061 ± 0.0029997\n"
     ]
    }
   ],
   "source": [
    "if epochs > 1:\n",
    "    mean_train_errors = np.array([x[\"train\"] for x in metrics]).mean(0)\n",
    "    mean_validation_errors = np.array([x[\"validation\"] for x in metrics]).mean(0)\n",
    "    plot_errors(mean_train_errors, mean_validation_errors)\n",
    "\n",
    "recalls = [x[\"recall\"] for x in metrics]\n",
    "print(f\"Mean recall @ {nr_recommendations} = {np.mean(recalls):.5} ± {np.std(recalls):.5}\")\n",
    "ndcgs = [x[\"ndcg\"] for x in metrics]\n",
    "print(f\"Mean NDCG @ {nr_recommendations} = {np.mean(ndcgs):.5} ± {np.std(ndcgs):.5}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}