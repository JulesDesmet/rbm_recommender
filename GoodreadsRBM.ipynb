{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter==1.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: numpy==1.21.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.21.3)\r\n",
      "Requirement already satisfied: matplotlib==3.4.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.4.3)\r\n",
      "Requirement already satisfied: pandas==1.3.4 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.3.4)\r\n",
      "Requirement already satisfied: scikit-learn==1.0.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.0.1)\r\n",
      "Requirement already satisfied: tensorflow==2.7.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: tqdm==4.62.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.62.3)\r\n",
      "Requirement already satisfied: ipywidgets in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (7.6.5)\r\n",
      "Requirement already satisfied: qtconsole in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.2.1)\r\n",
      "Requirement already satisfied: ipykernel in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.6.0)\r\n",
      "Requirement already satisfied: jupyter-console in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.0)\r\n",
      "Requirement already satisfied: notebook in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.6)\r\n",
      "Requirement already satisfied: nbconvert in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.8.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (3.0.6)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (0.11.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (8.4.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (1.3.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from pandas==1.3.4->-r requirements.txt (line 4)) (2021.3)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (3.0.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 5)) (1.1.0)\r\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.36.2)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.22.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (4.0.1)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.6.3)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.0)\r\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.2)\r\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.16.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.13.3)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.19.1)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (12.0.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.42.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.0.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.6.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (57.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.6)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.3.3)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.26.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.6.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.8.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.2.4)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.8.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.6.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2021.10.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.1.1)\r\n",
      "Requirement already satisfied: jupyter-client<8.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (7.1.0)\r\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (7.30.1)\r\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.3)\r\n",
      "Requirement already satisfied: traitlets<6.0,>=5.1.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (6.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (4.8.0)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.18.1)\r\n",
      "Requirement already satisfied: pickleshare in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.23)\r\n",
      "Requirement already satisfied: backcall in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: pygments in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (2.10.0)\r\n",
      "Requirement already satisfied: decorator in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.3)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.4)\r\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (4.9.1)\r\n",
      "Requirement already satisfied: pyzmq>=13 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (22.3.0)\r\n",
      "Requirement already satisfied: entrypoints in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /home/darin/rbm_3.9/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.5)\r\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (3.5.2)\r\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.3)\r\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (1.0.2)\r\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (4.2.1)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (21.2.0)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.18.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.3)\r\n",
      "Requirement already satisfied: argon2-cffi in /home/darin/rbm_3.9/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (21.1.0)\r\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: prometheus-client in /home/darin/rbm_3.9/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.1)\r\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /home/darin/rbm_3.9/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.21)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from jinja2->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.0.1)\r\n",
      "Requirement already satisfied: bleach in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (4.1.0)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.2)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.0)\r\n",
      "Requirement already satisfied: testpath in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.0)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.4)\r\n",
      "Requirement already satisfied: defusedxml in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/darin/rbm_3.9/lib/python3.9/site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.9)\r\n",
      "Requirement already satisfied: webencodings in /home/darin/rbm_3.9/lib/python3.9/site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: packaging in /home/darin/rbm_3.9/lib/python3.9/site-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: qtpy in /home/darin/rbm_3.9/lib/python3.9/site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (1.11.3)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/darin/rbm_3.9/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc6a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 10:23:05.961444: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-21 10:23:05.961499: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from IPython.display import display\n",
    "\n",
    "from json import loads\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Generator, Iterable, Union, TypeVar\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597fd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(filename: str, read_max: int = None, attributes: Iterable[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the file line by line, parsing each line as json.\n",
    "\n",
    "    :param filename: The path to the datafile.\n",
    "    :param read_max: The maximum number of lines to read from the datafile.\n",
    "    :param attributes: The attributes of each JSON object that should be extracted; other attributes are ignored.\n",
    "    \"\"\"\n",
    "    file = gzip.open(filename, \"r\")\n",
    "    data = []\n",
    "    for index, line in enumerate(tqdm(file)):\n",
    "        if index == read_max:\n",
    "            break\n",
    "        entry = loads(line)\n",
    "        if attributes is not None:\n",
    "            entry = {key: entry[key] for key in attributes}\n",
    "        data.append(entry)\n",
    "    return pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "637dad06f969447ab2ea60750283d251"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e141fe0768e4b0abe3aaa3620159353"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    book_id                                              title\n0  25742454                              The Switchblade Mamma\n1  30128855                                            Cruelle\n2  13571772  Captain America: Winter Soldier (The Ultimate ...\n3  35452242  Bounty Hunter 4/3: My Life in Combat from Mari...\n4    707611                          Superman Archives, Vol. 2\n5   2250580                            A.I. Revolution, Vol. 1\n6  27036536                              War Stories, Volume 3\n7  27036537                                 Crossed, Volume 15\n8  27036538  Crossed + One Hundred, Volume 2 (Crossed +100 #2)\n9  27036539                              War Stories, Volume 4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25742454</td>\n      <td>The Switchblade Mamma</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30128855</td>\n      <td>Cruelle</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13571772</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35452242</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>707611</td>\n      <td>Superman Archives, Vol. 2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2250580</td>\n      <td>A.I. Revolution, Vol. 1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>27036536</td>\n      <td>War Stories, Volume 3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27036537</td>\n      <td>Crossed, Volume 15</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27036538</td>\n      <td>Crossed + One Hundred, Volume 2 (Crossed +100 #2)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27036539</td>\n      <td>War Stories, Volume 4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d    836610       0\n1  8842281e1d1347389f2ab93d60773d4d   7648967       0\n2  8842281e1d1347389f2ab93d60773d4d  15704307       0\n3  8842281e1d1347389f2ab93d60773d4d   6902644       0\n4  8842281e1d1347389f2ab93d60773d4d   9844623       0\n5  8842281e1d1347389f2ab93d60773d4d  13163846       0\n6  8842281e1d1347389f2ab93d60773d4d   1137635       0\n7  8842281e1d1347389f2ab93d60773d4d     44735       0\n8  8842281e1d1347389f2ab93d60773d4d    472331       0\n9  8842281e1d1347389f2ab93d60773d4d     24815       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>13163846</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1137635</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>44735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>472331</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24815</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"data/\"\n",
    "books = f\"{data_path}goodreads_books_comics_graphic.json.gz\"\n",
    "interactions = f\"{data_path}goodreads_interactions_comics_graphic.json.gz\"\n",
    "reviews = f\"{data_path}goodreads_reviews_comics_graphic.json.gz\"\n",
    "\n",
    "n = None\n",
    "\n",
    "books_df = parse_json(books, n, (\"book_id\", \"title\"))\n",
    "interactions_df = parse_json(interactions, n, (\"user_id\", \"book_id\", \"rating\"))\n",
    "\n",
    "display(books_df.head(10))\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the columns to the correct types\n",
    "# interactions_df[\"date_updated\"] = pd.to_datetime(interactions_df[\"date_updated\"], format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "books_df[\"book_id\"] = books_df[\"book_id\"].astype(\"int64\")\n",
    "interactions_df[\"book_id\"] = interactions_df[\"book_id\"].astype(\"int64\")\n",
    "\n",
    "# Sort the interactions by user ID and the timestamp\n",
    "# interactions_df = interactions_df.sort_values(by=[\"user_id\", \"date_updated\"], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d    836610       0\n1  8842281e1d1347389f2ab93d60773d4d   7648967       0\n2  8842281e1d1347389f2ab93d60773d4d  15704307       0\n3  8842281e1d1347389f2ab93d60773d4d   6902644       0\n4  8842281e1d1347389f2ab93d60773d4d   9844623       0\n5  8842281e1d1347389f2ab93d60773d4d  13163846       0\n6  8842281e1d1347389f2ab93d60773d4d   1137635       0\n7  8842281e1d1347389f2ab93d60773d4d     44735       0\n8  8842281e1d1347389f2ab93d60773d4d    472331       0\n9  8842281e1d1347389f2ab93d60773d4d     24815       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>13163846</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1137635</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>44735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>472331</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24815</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 342415\n",
      "Number of unique items: 89411\n",
      "7347630 initial rows\n",
      "6995891 rows after preprocessing\n",
      "Number of unique users: 148438\n",
      "Number of unique items: 89276\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d    836610       0\n1  8842281e1d1347389f2ab93d60773d4d   7648967       0\n2  8842281e1d1347389f2ab93d60773d4d  15704307       0\n3  8842281e1d1347389f2ab93d60773d4d   6902644       0\n4  8842281e1d1347389f2ab93d60773d4d   9844623       0\n5  8842281e1d1347389f2ab93d60773d4d  13163846       0\n6  8842281e1d1347389f2ab93d60773d4d   1137635       0\n7  8842281e1d1347389f2ab93d60773d4d     44735       0\n8  8842281e1d1347389f2ab93d60773d4d    472331       0\n9  8842281e1d1347389f2ab93d60773d4d     24815       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>836610</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>7648967</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>15704307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>6902644</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>9844623</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>13163846</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1137635</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>44735</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>472331</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>24815</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(dataframe: pd.DataFrame, min_support: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes users with fewer than `min_support` interactions, and duplicate user-item pairs (which do not exist in the\n",
    "    dataset anyway). Items with very few interactions are not removed, unless they have no interactions at all after\n",
    "    removing infrequent users.\n",
    "    \"\"\"\n",
    "    print(dataframe.shape[0], \"initial rows\")\n",
    "    # Drop reconsumption items\n",
    "    dataframe = dataframe.drop_duplicates(subset=[\"user_id\", \"book_id\"])\n",
    "    # Drop users with less than `min_support` interactions\n",
    "    items_per_user = dataframe.groupby(\"user_id\", as_index=False)[\"book_id\"].size()\n",
    "    items_per_user = items_per_user.rename({\"size\": \"items_per_user\"}, axis=\"columns\")\n",
    "    dataframe = pd.merge(dataframe, items_per_user, how=\"left\", on=[\"user_id\"])\n",
    "    dataframe = dataframe[dataframe[\"items_per_user\"] >= min_support]\n",
    "    # Report and clean up after the preprocessing\n",
    "    print(dataframe.shape[0], \"rows after preprocessing\")\n",
    "    dataframe.drop(columns=[\"items_per_user\"], inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "display(interactions_df.head(10))\n",
    "print(f\"Number of unique users:\", interactions_df[\"user_id\"].nunique())\n",
    "print(f\"Number of unique items:\", interactions_df[\"book_id\"].nunique())\n",
    "interactions_df = preprocess(interactions_df, min_support=5)\n",
    "print(f\"Number of unique users:\", interactions_df[\"user_id\"].nunique())\n",
    "print(f\"Number of unique items:\", interactions_df[\"book_id\"].nunique())\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6995891 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a44f30afa244b3eae09a13141deb230"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6995891 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecbc4045cea240b19ef5fb010904c8f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/89411 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7591c5867fd34b8a93657ee201698e69"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "    book_id                                              title  book_id_int\n0  25742454                              The Switchblade Mamma        89095\n1  30128855                                            Cruelle        51852\n2  13571772  Captain America: Winter Soldier (The Ultimate ...        41242\n3  35452242  Bounty Hunter 4/3: My Life in Combat from Mari...        88960\n4    707611                          Superman Archives, Vol. 2        54601\n5   2250580                            A.I. Revolution, Vol. 1        28661\n6  27036536                              War Stories, Volume 3        11531\n7  27036537                                 Crossed, Volume 15        11422\n8  27036538  Crossed + One Hundred, Volume 2 (Crossed +100 #2)        11373\n9  27036539                              War Stories, Volume 4        10885",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>title</th>\n      <th>book_id_int</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25742454</td>\n      <td>The Switchblade Mamma</td>\n      <td>89095</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30128855</td>\n      <td>Cruelle</td>\n      <td>51852</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13571772</td>\n      <td>Captain America: Winter Soldier (The Ultimate ...</td>\n      <td>41242</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35452242</td>\n      <td>Bounty Hunter 4/3: My Life in Combat from Mari...</td>\n      <td>88960</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>707611</td>\n      <td>Superman Archives, Vol. 2</td>\n      <td>54601</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2250580</td>\n      <td>A.I. Revolution, Vol. 1</td>\n      <td>28661</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>27036536</td>\n      <td>War Stories, Volume 3</td>\n      <td>11531</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>27036537</td>\n      <td>Crossed, Volume 15</td>\n      <td>11422</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27036538</td>\n      <td>Crossed + One Hundred, Volume 2 (Crossed +100 #2)</td>\n      <td>11373</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27036539</td>\n      <td>War Stories, Volume 4</td>\n      <td>10885</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   user_id  item_id  rating\n0        0        0       0\n1        0        1       0\n2        0        2       0\n3        0        3       0\n4        0        4       0\n5        0        5       0\n6        0        6       0\n7        0        7       0\n8        0        8       0\n9        0        9       5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>item_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>9</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_consecutive_mapping(dataframe: pd.DataFrame, column: str, new_column: str, *additional: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generates a consecutive ID column for the values of an existing column. Also adds this column to additional data\n",
    "    frames with the exact same mapping of old ID to new (consecutive) ID.\n",
    "    \"\"\"\n",
    "    ids = {}\n",
    "\n",
    "    def map_to_consecutive_ids(uuid: Union[int, np.int64]) -> int:\n",
    "        \"\"\"\n",
    "        To be used with `pd.Dataframe.apply()` or `pd.Dataframe.progress_apply()`; returns a unique ID per distinct\n",
    "        value.\n",
    "        \"\"\"\n",
    "        if uuid not in ids:\n",
    "            ids[uuid] = len(ids)\n",
    "        return ids[uuid]\n",
    "\n",
    "    dataframe[new_column] = dataframe[column].progress_apply(map_to_consecutive_ids)\n",
    "    for frame in additional:\n",
    "        frame[new_column] = frame[column].progress_apply(lambda old_id: ids.get(old_id, -1))\n",
    "\n",
    "\n",
    "apply_consecutive_mapping(interactions_df, \"user_id\", \"user_id_int\")\n",
    "apply_consecutive_mapping(interactions_df, \"book_id\", \"book_id_int\", books_df)\n",
    "\n",
    "interactions_df = interactions_df[[\"user_id_int\", \"book_id_int\", \"rating\"]]\n",
    "interactions_df = interactions_df.rename(columns={\"user_id_int\": \"user_id\", \"book_id_int\": \"item_id\"})\n",
    "\n",
    "display(books_df.head(10))\n",
    "display(interactions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataType = TypeVar(\"DataType\", pd.DataFrame, sparse.csr_matrix)\n",
    "\n",
    "\n",
    "def generate_random_split(data: DataType, seed: int = None) -> tuple:\n",
    "    return train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "def split_k_folds(data: sparse.csr_matrix, nr_folds: int = 5) -> Generator[tuple, None, None]:\n",
    "    \"\"\"\n",
    "    Generates K train-test splits for K-fold cross validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nr_folds)\n",
    "    yield from kf.split(data)\n",
    "\n",
    "\n",
    "def create_sparse_matrix(dataframe: pd.DataFrame, shape: tuple = None) -> sparse.csr_matrix:\n",
    "    return sparse.csr_matrix((dataframe[\"rating\"], (dataframe[\"user_id\"], dataframe[\"item_id\"])), shape=shape,\n",
    "                             dtype=np.int8)\n",
    "\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X) -> tf.SparseTensor:\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, np.float32(coo.data), coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shape = (interactions_df[\"user_id\"].max() + 1, interactions_df[\"item_id\"].max() + 1)\n",
    "interactions_df[\"rating\"] = interactions_df[\"rating\"].apply(lambda value: (value + 1) / 6.0)\n",
    "# interaction_matrix = create_sparse_matrix(interactions_df, shape)\n",
    "# normalized_matrix = interaction_matrix\n",
    "# normalized_matrix.data = (interaction_matrix.data / 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We then move on to creating the visible and hidden layer units and setting their activation functions. In this case, we\n",
    "will be using the <code>tf.sigmoid</code> and <code>tf.relu</code> functions as nonlinear activations since it is\n",
    "commonly used in RBM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input processing: Defining a function to return only the generated hidden states\n",
    "\n",
    "def hidden_layer(v0_state, W, hb):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + hb)\n",
    "    return tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))\n",
    "\n",
    "\n",
    "def reconstructed_output(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)\n",
    "    return tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))\n",
    "\n",
    "\n",
    "def gibbs_sampling(v0_state: tf.Tensor, weights: tf.Variable, bias_visible: tf.Variable, bias_hidden: tf.Variable,\n",
    "                   nr_iterations: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs gibbs sampling starting with the given input vector.\n",
    "    :return: A tuple consisting of (in order):\n",
    "        - The hidden layer `h0_state`, sampled from the given data vector `v0_state`.\n",
    "        - The final visible layer `vk_state`.\n",
    "        - The final hidden layer `hk_state`.\n",
    "    \"\"\"\n",
    "    h0_state = hidden_layer(v0_state, weights, hb)\n",
    "    vk_state = v0_state\n",
    "    hk_state = h0_state\n",
    "    for _ in range(nr_iterations):\n",
    "        vk_state = reconstructed_output(hk_state, weights, bias_visible)\n",
    "        hk_state = hidden_layer(vk_state, weights, bias_hidden)\n",
    "    return h0_state, vk_state, hk_state\n",
    "\n",
    "def get_k_recommendations(user_input: tf.Tensor, W: tf.Tensor, hb: tf.Tensor, vb: tf.Tensor,\n",
    "                          k: int) -> sparse.csr_matrix:\n",
    "    user_input = tf.convert_to_tensor(user_input, \"float32\")\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(user_input, W) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    scores = vv1.numpy()\n",
    "    scores[user_input.numpy() > 0] = 0\n",
    "    recommendations = np.argpartition(-scores, k, 1)[:, :k]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conditional RBM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def hidden_layer_conditional(v0_state, W, hb, r, D):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + tf.matmul(r, D) + hb)\n",
    "    return tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))\n",
    "\n",
    "\n",
    "def reconstructed_output_conditional(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)\n",
    "    return tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))\n",
    "\n",
    "\n",
    "def gibbs_sampling_conditional(v0_state: tf.Tensor, weights: tf.Variable, r: tf.Tensor, cond_weights: tf.Variable,\n",
    "                   bias_visible: tf.Variable, bias_hidden: tf.Variable, nr_iterations: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs gibbs sampling starting with the given input vector.\n",
    "    :return: A tuple consisting of (in order):\n",
    "        - The hidden layer `h0_state`, sampled from the given data vector `v0_state`.\n",
    "        - The final visible layer `vk_state`.\n",
    "        - The final hidden layer `hk_state`.\n",
    "    \"\"\"\n",
    "    h0_state = hidden_layer_conditional(v0_state, weights, hb, r, cond_weights)\n",
    "    vk_state = v0_state\n",
    "    hk_state = h0_state\n",
    "    for _ in range(nr_iterations):\n",
    "        vk_state = reconstructed_output_conditional(hk_state, weights, bias_visible)\n",
    "        hk_state = hidden_layer_conditional(vk_state, weights, bias_hidden, r, cond_weights)\n",
    "    return h0_state, vk_state, hk_state\n",
    "\n",
    "def get_k_recommendations_conditional(user_input: tf.Tensor, r: tf.Tensor, W: tf.Tensor, D: tf.Tensor, hb: tf.Tensor,\n",
    "                                      vb: tf.Tensor, k: int) -> sparse.csr_matrix:\n",
    "    user_input = tf.convert_to_tensor(user_input, \"float32\")\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(user_input, W) + tf.matmul(r, D) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    scores = vv1.numpy()\n",
    "    scores[user_input.numpy() > 0] = 0\n",
    "    recommendations = np.argpartition(-scores, k, 1)[:, :k]\n",
    "    return recommendations\n",
    "\n",
    "def get_r(data: tf.SparseTensor) -> tf.Tensor:\n",
    "    values = np.ones(len(data.indices))\n",
    "    indices = data.indices.numpy().T\n",
    "    sparse_r = sparse.coo_matrix((values, indices), shape=data.shape)\n",
    "    return tf.constant(sparse_r.toarray(), tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error(v0_state, v1_state):\n",
    "    \"\"\"\n",
    "    Returns the sum of the squared recons# Multiplies the explainability score by 5 to give a rating out of 5 for each item\n",
    "for rec in range(len(recommendations[0])):\n",
    "    if explainability_scores[rec] > 0:\n",
    "        print(\"Item with ID: \" + str(recommendations[0][rec])+' has been rated a '+ str(math.ceil(explainability_scores[rec]*5))+ ' out of 5 by users similar to you')truction errors. This error is computed per batch, and should be accumulated\n",
    "    per epoch. At the end of the epoch the total RMSE can then be computed from that sum.\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(tf.reduce_mean(tf.square(v0_state - v1_state), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def array_to_sparse_matrix(recommendations: np.ndarray) -> sparse.csr_matrix:\n",
    "    user_ids = np.repeat(np.arange(shape[0]), recommendations.shape[1])\n",
    "    item_ids = recommendations.flatten()\n",
    "    scores = np.ones(shape[0] * recommendations.shape[1])\n",
    "    return sparse.csr_matrix((scores, (user_ids, item_ids)), shape=shape)\n",
    "\n",
    "\n",
    "def sparse_invert_nonzero(a: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    inverse = a.copy()\n",
    "    inverse.data = 1 / inverse.data\n",
    "    return inverse\n",
    "\n",
    "\n",
    "def sparse_divide_nonzero(a: sparse.csr_matrix, b: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    return a.multiply(sparse_invert_nonzero(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def compute_recall(true: sparse.csr_matrix, predicted: sparse.csr_matrix) -> float:\n",
    "    scores = sparse.lil_matrix(predicted.shape)\n",
    "    scores[predicted.multiply(true).astype(bool)] = 1\n",
    "    scores = sparse_divide_nonzero(scores.tocsr(), sparse.csr_matrix(true.sum(axis=1))).sum(axis=1)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def get_ndcg(top_k: np.ndarray, actual_scores: np.ndarray):\n",
    "    numerator = actual_scores[top_k]\n",
    "    denom = np.log2(np.arange(2, len(top_k) + 2))\n",
    "    # Calculate DCG and IDCG\n",
    "    dcg = np.divide(numerator, denom).sum()\n",
    "    if dcg == 0:\n",
    "        return 0\n",
    "    # Sort the scores based on relevance\n",
    "    ideal_numerator = np.sort(numerator)[::-1]\n",
    "    idcg = np.divide(ideal_numerator, denom).sum()\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_rbm(train: tf.data.Dataset, validation: tf.data.Dataset, weights: tf.Variable, vb: tf.Variable,\n",
    "              hb: tf.Variable, cond_weights: tf.Variable = None) -> tuple:\n",
    "    \"\"\"\n",
    "    The training loop of the (conditional) RBM.\n",
    "\n",
    "    :param train: The training dataset.\n",
    "    :param validation: The validation dataset.\n",
    "    :param weights: The weights of the RBM.\n",
    "    :param vb: The biases of the visible layer of the RBM.\n",
    "    :param hb: The biases of the hidden layer of the RBM.\n",
    "    :param cond_weights: The interaction vector weights of the conditional RBM.\n",
    "\n",
    "    :return: A tuple containing the final weights, biases, training errors (RMSE), and validation errors.\n",
    "    \"\"\"\n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "\n",
    "    for _ in trange(epochs, leave=False):\n",
    "        train_errors.append(0)\n",
    "        train_iter = iter(train)\n",
    "        for _ in trange(len(train), leave=False):\n",
    "            batch = next(train_iter)\n",
    "            v0_state = tf.sparse.to_dense(batch)\n",
    "\n",
    "            if cond_weights is None:\n",
    "                h0_state, vk_state, hk_state = gibbs_sampling(v0_state, weights, vb, hb, gibbs_sampling_iterations)\n",
    "            else:\n",
    "                r = get_r(batch)\n",
    "                h0_state, vk_state, hk_state = gibbs_sampling_conditional(\n",
    "                    v0_state, weights, r, cond_weights, vb, hb, gibbs_sampling_iterations)\n",
    "\n",
    "            temp_0 = tf.matmul(tf.transpose(v0_state), tf.squeeze(h0_state))\n",
    "            temp_k = tf.matmul(tf.transpose(vk_state), tf.squeeze(hk_state))\n",
    "            delta = (temp_0 - temp_k) / v0_state.shape[0]\n",
    "\n",
    "            weights = weights + alpha * delta\n",
    "            vb = vb + alpha * tf.reduce_mean(v0_state - vk_state, 0)\n",
    "            hb = hb + alpha * tf.reduce_mean(tf.squeeze(h0_state - hk_state), 0)\n",
    "            if cond_weights is not None:\n",
    "                cond_weights = cond_weights + alpha * tf.matmul(tf.transpose(r), h0_state - hk_state)\n",
    "\n",
    "            train_errors[-1] += error(v0_state, vk_state)\n",
    "\n",
    "        validation_errors.append(0)\n",
    "        validation_iter = iter(validation)\n",
    "        for _ in trange(len(validation), leave=False):\n",
    "            batch = next(validation_iter)\n",
    "            v0_state = tf.sparse.to_dense(batch)\n",
    "            if cond_weights is None:\n",
    "                _, vk_state, _ = gibbs_sampling(v0_state, weights, vb, hb, 1)\n",
    "            else:\n",
    "                _, vk_state, _ = gibbs_sampling_conditional(v0_state, weights, get_r(batch), cond_weights, vb, hb, 1)\n",
    "            validation_errors[-1] += error(v0_state, vk_state)\n",
    "\n",
    "        train_errors[-1] = (train_errors[-1] / len(train)) ** 0.5\n",
    "        validation_errors[-1] = (validation_errors[-1] / len(validation)) ** 0.5\n",
    "        # print(f\"Epoch {epoch + 1:3}:  {train_errors[-1]:10.5}  ;  {validation_errors[-1]:10.5}\")\n",
    "\n",
    "    return weights, cond_weights, vb, hb, train_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_errors(train_errors: list, validation_errors: list, file_name: str = None) -> None:\n",
    "    plt.plot(range(1, epochs + 1), train_errors, label=\"Train\")\n",
    "    plt.plot(range(1, epochs + 1), validation_errors, label=\"Validation\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    if file_name is not None:\n",
    "        plt.savefig(\"rmse.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 10:28:24.037930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-21 10:28:24.037986: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-21 10:28:24.038028: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (darin-VirtualBox): /proc/driver/nvidia/version does not exist\n",
      "2021-12-21 10:28:24.059273: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "gibbs_sampling_iterations = 1\n",
    "nr_recommendations = 10\n",
    "alpha = 0.01  # Learning rate\n",
    "\n",
    "hiddenUnits = 1000\n",
    "visibleUnits = shape[1]\n",
    "\n",
    "rng = tf.random.Generator.from_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87edb584e6bf49748dad38931de5d21e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "685be97f2527471eb7e27d1878f2e6cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1188 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b58aeacab2844cf97d4c75017fbb0f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_6440/3052987503.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0mhb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVariable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrng\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mhiddenUnits\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m         \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_errors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_errors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_rbm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_6440/2493981988.py\u001B[0m in \u001B[0;36mtrain_rbm\u001B[0;34m(train, validation, weights, vb, hb, cond_weights)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m             \u001B[0mtemp_0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv0_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh0_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m             \u001B[0mtemp_k\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvk_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhk_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m             \u001B[0mdelta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtemp_0\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mtemp_k\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mv0_state\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/rbm_3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/rbm_3.9/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mop_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1094\u001B[0m       \u001B[0;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1095\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1096\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1097\u001B[0m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1098\u001B[0m         \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/rbm_3.9/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mmatmul\u001B[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001B[0m\n\u001B[1;32m   3698\u001B[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001B[1;32m   3699\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3700\u001B[0;31m         return gen_math_ops.mat_mul(\n\u001B[0m\u001B[1;32m   3701\u001B[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001B[1;32m   3702\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/rbm_3.9/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mmat_mul\u001B[0;34m(a, b, transpose_a, transpose_b, name)\u001B[0m\n\u001B[1;32m   6011\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6012\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6013\u001B[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[1;32m   6014\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"MatMul\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"transpose_a\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtranspose_a\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"transpose_b\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6015\u001B[0m         transpose_b)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "W, D, vb, hb = None, None, None, None\n",
    "\n",
    "for seed in trange(1, 6, leave=False):\n",
    "    # train, test = generate_random_split(normalized_matrix, seed=seed)\n",
    "    train_df, test_df = generate_random_split(interactions_df, seed=seed)\n",
    "    train = create_sparse_matrix(train_df, shape)\n",
    "    test = create_sparse_matrix(test_df, shape)\n",
    "\n",
    "    mean_train_errors = [0] * epochs\n",
    "    mean_validation_errors = [0] * epochs\n",
    "\n",
    "    for train_indices, validation_indices in split_k_folds(train):\n",
    "        tensor_train = convert_sparse_matrix_to_sparse_tensor(train[train_indices])\n",
    "        tensor_validation = convert_sparse_matrix_to_sparse_tensor(train[validation_indices])\n",
    "\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices(tensor_train).batch(batch_size)\n",
    "        validation_ds = tf.data.Dataset.from_tensor_slices(tensor_validation).batch(batch_size)\n",
    "\n",
    "        # W = tf.Variable(tf.zeros([visibleUnits, hiddenUnits]), tf.float32)\n",
    "        # D = tf.Variable(tf.zeros([visibleUnits, hiddenUnits]), tf.bool)\n",
    "        # vb = tf.Variable(tf.zeros([visibleUnits]), tf.float32)\n",
    "        # hb = tf.Variable(tf.zeros([hiddenUnits]), tf.float32)\n",
    "\n",
    "        W = tf.Variable(rng.normal([visibleUnits, hiddenUnits], 0, 0.1), tf.float32)\n",
    "        vb = tf.Variable(rng.normal([visibleUnits], 0, 0.1), tf.float32)\n",
    "        hb = tf.Variable(rng.normal([hiddenUnits], 0, 0.1), tf.float32)\n",
    "\n",
    "        W, _, vb, hb, train_errors, validation_errors = train_rbm(train_ds, validation_ds, W, vb, hb)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            mean_train_errors[epoch] += train_errors[epoch] / 5\n",
    "            mean_validation_errors[epoch] += validation_errors[epoch] / 5\n",
    "\n",
    "        break\n",
    "\n",
    "    plot_errors(mean_train_errors, mean_validation_errors)\n",
    "\n",
    "    recommendations = np.empty((shape[0], 10), dtype=np.int_)\n",
    "    for batch_start in trange(0, train.shape[0], batch_size, leave=False):\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch = train[batch_start: batch_end]\n",
    "        # r = get_r(convert_sparse_matrix_to_sparse_tensor(batch))\n",
    "        recommendations[batch_start: batch_end] = get_k_recommendations(batch.toarray(), W, hb, vb, nr_recommendations)\n",
    "    recall = compute_recall(test, array_to_sparse_matrix(recommendations))\n",
    "\n",
    "    print(\"Recall @\", nr_recommendations, \"=\", recall)\n",
    "\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "# Creates neighbors for specific user based on cosine similarity\n",
    "def create_neighbors(user: int):\n",
    "    mock_user = interactions_df.loc[interactions_df['user_id'] == user]\n",
    "    items_user = mock_user['item_id'].values\n",
    "    neighbors = []\n",
    "    for i in trange(len(interactions_df['user_id'].unique())):\n",
    "        value = interactions_df.loc[interactions_df['user_id'] == i]\n",
    "        items = value['item_id'].values\n",
    "        denominator = math.sqrt(len(items_user))+math.sqrt(len(items))\n",
    "        numerator = len(np.intersect1d(items,items_user))\n",
    "        similarity = numerator/denominator\n",
    "        if similarity > 0.5:\n",
    "            neighbors.append(i)\n",
    "    return neighbors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get recommendations for this user, in this case we knew that the ratings for user 1 were all 0 so we did it in a bit of a hacky way and created a tensor flow with 89276 zeros, for other users we'll need to fill this with 1's in places where the user rated a book.\n",
    "tensor_user = tf.zeros([1,89276], tf.float32)\n",
    "recommendations = get_k_recommendations(tensor_user, W=W, hb=hb, vb=vb, k=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculates the explainability score for each recommendation\n",
    "neighbors = create_neighbors(1)\n",
    "def create_explainability_scores(recommendations, neighbors):\n",
    "    explainability_scores = []\n",
    "    for item in recommendations[0]:\n",
    "        score = 0\n",
    "        nom = 0\n",
    "        max_score = 0\n",
    "        for neighbor in neighbors:\n",
    "            current_neighbor = interactions_df.loc[interactions_df['user_id'] == neighbor]\n",
    "            if item in current_neighbor['item_id'].values:\n",
    "                rating = current_neighbor.loc[current_neighbor['item_id'] == item]['rating'].values[0]\n",
    "                nom+=rating\n",
    "                max_score = max(max_score,rating)\n",
    "        if max_score == 0:\n",
    "            explainability_scores.append(0)\n",
    "        else:\n",
    "            score = nom/(len(neighbors)*max_score)\n",
    "            explainability_scores.append(score)\n",
    "\n",
    "    # Multiplies the explainability score by 5 to give a rating out of 5 for each item\n",
    "\n",
    "    for rec in range(len(recommendations[0])):\n",
    "        if explainability_scores[rec] > 0:\n",
    "            print(\"Item with ID: \" + str(recommendations[0][rec])+' has been rated a '+ str(math.ceil(explainability_scores[rec]*5))+ ' out of 5 by users similar to you')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = create_explainability_scores(recommendations,neighbors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}