{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.0.0)\r\n",
      "Requirement already satisfied: numpy==1.21.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.21.3)\r\n",
      "Requirement already satisfied: matplotlib==3.4.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.4.3)\r\n",
      "Collecting pandas==1.3.4\r\n",
      "  Downloading pandas-1.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 11.5 MB 8.6 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting scikit-learn==1.0.1\r\n",
      "  Downloading scikit_learn-1.0.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 25.9 MB 11.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (2.7.0)\r\n",
      "Collecting tqdm==4.62.3\r\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 76 kB 5.2 MB/s  eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (7.6.5)\r\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.2.0)\r\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.5)\r\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (6.4.0)\r\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.8/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (1.3.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (2.8.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.4.3->-r requirements.txt (line 3)) (8.4.0)\r\n",
      "Collecting pytz>=2017.3\r\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 503 kB 10.2 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting scipy>=1.1.0\r\n",
      "  Downloading scipy-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.3 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 39.3 MB 3.6 MB/s eta 0:00:013\r\n",
      "\u001B[?25hCollecting joblib>=0.11\r\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 306 kB 10.1 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting threadpoolctl>=2.0.0\r\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.19.1)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.13.3)\r\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.5.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.14.0)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (12.0.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/lib/python3/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.34.2)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.2)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.21.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.15.0)\r\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.6.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (3.10.0.2)\r\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.0)\r\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (2.7.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.41.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r requirements.txt (line 6)) (1.1.0)\r\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (1.0.2)\r\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (7.29.0)\r\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (4.4.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.1)\r\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (3.5.2)\r\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (7.0.6)\r\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (6.1)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.0)\r\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.0)\r\n",
      "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (2.10.0)\r\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.3)\r\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.4)\r\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (4.9.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.2)\r\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (4.1.0)\r\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.2)\r\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.8.4)\r\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (21.1.0)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (22.3.0)\r\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.8.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.1)\r\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.12.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (3.0.22)\r\n",
      "Requirement already satisfied: qtpy in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 1)) (1.11.2)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.8.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.22.0)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (58.5.2)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.3.4)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.3.3)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (2.0.2)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.6)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.6.1)\r\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (4.8.0)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (5.1.0)\r\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.1.3)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.17.2)\r\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.5)\r\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (4.2.0)\r\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->-r requirements.txt (line 1)) (1.5.1)\r\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (0.5.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (21.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 1)) (2.0.1)\r\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (1.15.0)\r\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 1)) (0.2.5)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.7.2)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (4.2.4)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (1.3.0)\r\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.7.1)\r\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (5.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (21.2.0)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (0.18.0)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter==1.0.0->-r requirements.txt (line 1)) (2.20)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r requirements.txt (line 6)) (3.1.1)\r\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources; python_version < \"3.9\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->-r requirements.txt (line 1)) (3.6.0)\r\n",
      "Installing collected packages: pytz, pandas, scipy, joblib, threadpoolctl, scikit-learn, tqdm\r\n",
      "Successfully installed joblib-1.1.0 pandas-1.3.4 pytz-2021.3 scikit-learn-1.0.1 scipy-1.7.3 threadpoolctl-3.0.0 tqdm-4.62.3\r\n",
      "\u001B[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc6a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "from IPython.display import display\n",
    "\n",
    "from json import loads\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Generator, Iterable, Union, TypeVar\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def parse_json(filename: str, read_max: int = None, attributes: Iterable[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the file line by line, parsing each line as json.\n",
    "\n",
    "    :param filename: The path to the datafile.\n",
    "    :param read_max: The maximum number of lines to read from the datafile.\n",
    "    :param attributes: The attributes of each JSON object that should be extracted; other attributes are ignored.\n",
    "    \"\"\"\n",
    "    file = gzip.open(filename, \"r\")\n",
    "    data = []\n",
    "    for index, line in enumerate(tqdm(file)):\n",
    "        if index == read_max:\n",
    "            break\n",
    "        entry = loads(line)\n",
    "        if attributes is not None:\n",
    "            entry = {key: entry[key] for key in attributes}\n",
    "        data.append(entry)\n",
    "    return pd.DataFrame.from_dict(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f66e43275e5f44e2a235e7c592330fbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"data/\"\n",
    "books = f\"{data_path}goodreads_books_comics_graphic.json.gz\"\n",
    "interactions = f\"{data_path}goodreads_interactions_comics_graphic.json.gz\"\n",
    "reviews = f\"{data_path}goodreads_reviews_comics_graphic.json.gz\"\n",
    "\n",
    "n = None\n",
    "\n",
    "interactions_df = parse_json(interactions, n, (\"user_id\", \"book_id\", \"rating\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataType = TypeVar(\"DataType\", pd.DataFrame, sparse.csr_matrix)\n",
    "\n",
    "\n",
    "def generate_random_split(data: DataType, seed: int = None) -> tuple:\n",
    "    return train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "\n",
    "def split_k_folds(data: sparse.csr_matrix, nr_folds: int = 5) -> Generator[tuple, None, None]:\n",
    "    \"\"\"\n",
    "    Generates K train-test splits for K-fold cross validation.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nr_folds)\n",
    "    yield from kf.split(data)\n",
    "\n",
    "\n",
    "def create_sparse_matrix(dataframe: pd.DataFrame, shape: tuple = None) -> sparse.csr_matrix:\n",
    "    return sparse.csr_matrix((dataframe[\"rating\"], (dataframe[\"user_id\"], dataframe[\"item_id\"])), shape=shape,\n",
    "                             dtype=np.int8)\n",
    "\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X) -> tf.SparseTensor:\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, np.float32(coo.data), coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "interactions_df[\"rating\"] = interactions_df[\"rating\"].apply(lambda value: (value + 1) / 6.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We then move on to creating the visible and hidden layer units and setting their activation functions. In this case, we\n",
    "will be using the <code>tf.sigmoid</code> and <code>tf.relu</code> functions as nonlinear activations since it is\n",
    "commonly used in RBM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Input processing: Defining a function to return only the generated hidden states\n",
    "\n",
    "def hidden_layer(v0_state, W, hb):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + hb)\n",
    "    return tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))\n",
    "\n",
    "\n",
    "def reconstructed_output(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)\n",
    "    return tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))\n",
    "\n",
    "\n",
    "def gibbs_sampling(v0_state: tf.Tensor, weights: tf.Variable, bias_visible: tf.Variable, bias_hidden: tf.Variable,\n",
    "                   nr_iterations: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs gibbs sampling starting with the given input vector.\n",
    "    :return: A tuple consisting of (in order):\n",
    "        - The hidden layer `h0_state`, sampled from the given data vector `v0_state`.\n",
    "        - The final visible layer `vk_state`.\n",
    "        - The final hidden layer `hk_state`.\n",
    "    \"\"\"\n",
    "    h0_state = hidden_layer(v0_state, weights, hb)\n",
    "    vk_state = v0_state\n",
    "    hk_state = h0_state\n",
    "    for _ in range(nr_iterations):\n",
    "        vk_state = reconstructed_output(hk_state, weights, bias_visible)\n",
    "        hk_state = hidden_layer(vk_state, weights, bias_hidden)\n",
    "    return h0_state, vk_state, hk_state\n",
    "\n",
    "def get_k_recommendations(user_input: tf.Tensor, W: tf.Tensor, hb: tf.Tensor, vb: tf.Tensor,\n",
    "                          k: int) -> sparse.csr_matrix:\n",
    "    user_input = tf.convert_to_tensor(user_input, \"float32\")\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(user_input, W) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    scores = vv1.numpy()\n",
    "    scores[user_input.numpy() > 0] = 0\n",
    "    recommendations = np.argpartition(-scores, k, 1)[:, :k]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conditional RBM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def hidden_layer_conditional(v0_state, W, hb, r, D):\n",
    "    h0_prob = tf.nn.sigmoid(tf.matmul(v0_state, W) + tf.matmul(r, D) + hb)\n",
    "    return tf.nn.relu(tf.sign(h0_prob - tf.random.uniform(tf.shape(h0_prob))))\n",
    "\n",
    "\n",
    "def reconstructed_output_conditional(h0_state, W, vb):\n",
    "    v1_prob = tf.nn.sigmoid(tf.matmul(h0_state, tf.transpose(W)) + vb)\n",
    "    return tf.nn.relu(tf.sign(v1_prob - tf.random.uniform(tf.shape(v1_prob))))\n",
    "\n",
    "\n",
    "def gibbs_sampling_conditional(v0_state: tf.Tensor, weights: tf.Variable, r: tf.Tensor, cond_weights: tf.Variable,\n",
    "                   bias_visible: tf.Variable, bias_hidden: tf.Variable, nr_iterations: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs gibbs sampling starting with the given input vector.\n",
    "    :return: A tuple consisting of (in order):\n",
    "        - The hidden layer `h0_state`, sampled from the given data vector `v0_state`.\n",
    "        - The final visible layer `vk_state`.\n",
    "        - The final hidden layer `hk_state`.\n",
    "    \"\"\"\n",
    "    h0_state = hidden_layer_conditional(v0_state, weights, hb, r, cond_weights)\n",
    "    vk_state = v0_state\n",
    "    hk_state = h0_state\n",
    "    for _ in range(nr_iterations):\n",
    "        vk_state = reconstructed_output_conditional(hk_state, weights, bias_visible)\n",
    "        hk_state = hidden_layer_conditional(vk_state, weights, bias_hidden, r, cond_weights)\n",
    "    return h0_state, vk_state, hk_state\n",
    "\n",
    "def get_k_recommendations_conditional(user_input: tf.Tensor, r: tf.Tensor, W: tf.Tensor, D: tf.Tensor, hb: tf.Tensor,\n",
    "                                      vb: tf.Tensor, k: int) -> sparse.csr_matrix:\n",
    "    user_input = tf.convert_to_tensor(user_input, \"float32\")\n",
    "    hh0 = tf.nn.sigmoid(tf.matmul(user_input, W) + tf.matmul(r, D) + hb)\n",
    "    vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "    scores = vv1.numpy()\n",
    "    scores[user_input.numpy() > 0] = 0\n",
    "    recommendations = np.argpartition(-scores, k, 1)[:, :k]\n",
    "    return recommendations\n",
    "\n",
    "def get_r(data: tf.SparseTensor) -> tf.Tensor:\n",
    "    values = np.ones(len(data.indices))\n",
    "    indices = data.indices.numpy().T\n",
    "    sparse_r = sparse.coo_matrix((values, indices), shape=data.shape)\n",
    "    return tf.constant(sparse_r.toarray(), tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def error(v0_state, v1_state):\n",
    "    \"\"\"\n",
    "    Returns the sum of the squared reconstruction errors. This error is computed per batch, and should be accumulated\n",
    "    per epoch. At the end of the epoch the total RMSE can then be computed from that sum.\n",
    "    \"\"\"\n",
    "    return tf.reduce_sum(tf.reduce_mean(tf.square(v0_state - v1_state), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def array_to_sparse_matrix(recommendations: np.ndarray) -> sparse.csr_matrix:\n",
    "    user_ids = np.repeat(np.arange(shape[0]), recommendations.shape[1])\n",
    "    item_ids = recommendations.flatten()\n",
    "    scores = np.ones(shape[0] * recommendations.shape[1])\n",
    "    return sparse.csr_matrix((scores, (user_ids, item_ids)), shape=shape)\n",
    "\n",
    "\n",
    "def sparse_invert_nonzero(a: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    inverse = a.copy()\n",
    "    inverse.data = 1 / inverse.data\n",
    "    return inverse\n",
    "\n",
    "\n",
    "def sparse_divide_nonzero(a: sparse.csr_matrix, b: sparse.csr_matrix) -> sparse.csr_matrix:\n",
    "    return a.multiply(sparse_invert_nonzero(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def compute_recall(true: sparse.csr_matrix, predicted: sparse.csr_matrix) -> float:\n",
    "    scores = sparse.lil_matrix(predicted.shape)\n",
    "    scores[predicted.multiply(true).astype(bool)] = 1\n",
    "    scores = sparse_divide_nonzero(scores.tocsr(), sparse.csr_matrix(true.sum(axis=1))).sum(axis=1)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def get_ndcg(top_k: np.ndarray, actual_scores: np.ndarray):\n",
    "    numerator = actual_scores[top_k]\n",
    "    denom = np.log2(np.arange(2, len(top_k) + 2))\n",
    "    # Calculate DCG and IDCG\n",
    "    dcg = np.divide(numerator, denom).sum()\n",
    "    if dcg == 0:\n",
    "        return 0\n",
    "    # Sort the scores based on relevance\n",
    "    ideal_numerator = np.sort(numerator)[::-1]\n",
    "    idcg = np.divide(ideal_numerator, denom).sum()\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def compute_ndcg(true: sparse.csr_matrix, recommendations: np.ndarray) -> float:\n",
    "    ndcg_sum = 0\n",
    "    for user_id in trange(true.shape[0], leave=False):\n",
    "        ndcg_sum += get_ndcg(recommendations[user_id], true[user_id].toarray()[0])\n",
    "    return ndcg_sum / true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training the RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_rbm(train: tf.data.Dataset, validation: None, weights: tf.Variable, vb: tf.Variable,\n",
    "              hb: tf.Variable, cond_weights: tf.Variable = None) -> tuple:\n",
    "    \"\"\"\n",
    "    The training loop of the (conditional) RBM.\n",
    "\n",
    "    :param train: The training dataset.\n",
    "    :param validation: The validation dataset.\n",
    "    :param weights: The weights of the RBM.\n",
    "    :param vb: The biases of the visible layer of the RBM.\n",
    "    :param hb: The biases of the hidden layer of the RBM.\n",
    "    :param cond_weights: The interaction vector weights of the conditional RBM.\n",
    "\n",
    "    :return: A tuple containing the final weights, biases, training errors (RMSE), and validation errors.\n",
    "    \"\"\"\n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "\n",
    "    for _ in trange(epochs, leave=False):\n",
    "        train_errors.append(0)\n",
    "        train_iter = iter(train)\n",
    "        for _ in trange(len(train), leave=False):\n",
    "            batch = next(train_iter)\n",
    "            v0_state = tf.sparse.to_dense(batch)\n",
    "\n",
    "            if cond_weights is None:\n",
    "                h0_state, vk_state, hk_state = gibbs_sampling(v0_state, weights, vb, hb, gibbs_sampling_iterations)\n",
    "            else:\n",
    "                r = get_r(batch)\n",
    "                h0_state, vk_state, hk_state = gibbs_sampling_conditional(\n",
    "                    v0_state, weights, r, cond_weights, vb, hb, gibbs_sampling_iterations)\n",
    "\n",
    "            temp_0 = tf.matmul(tf.transpose(v0_state), tf.squeeze(h0_state))\n",
    "            temp_k = tf.matmul(tf.transpose(vk_state), tf.squeeze(hk_state))\n",
    "            delta = (temp_0 - temp_k) / v0_state.shape[0]\n",
    "\n",
    "            weights = weights + alpha * delta\n",
    "            vb = vb + alpha * tf.reduce_mean(v0_state - vk_state, 0)\n",
    "            hb = hb + alpha * tf.reduce_mean(tf.squeeze(h0_state - hk_state), 0)\n",
    "            if cond_weights is not None:\n",
    "                cond_weights = cond_weights + alpha * tf.matmul(tf.transpose(r), h0_state - hk_state)\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1:3}:  {train_errors[-1]:10.5}  ;  {validation_errors[-1]:10.5}\")\n",
    "\n",
    "    return weights, cond_weights, vb, hb, train_errors, validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_errors(train_errors: list, validation_errors: list, file_name: str = None) -> None:\n",
    "    plt.plot(range(1, epochs + 1), train_errors, label=\"Train\")\n",
    "    plt.plot(range(1, epochs + 1), validation_errors, label=\"Validation\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    if file_name is not None:\n",
    "        plt.savefig(\"rmse.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "interactions_df = interactions_df.rename(columns={\"user_id\": \"old_user\", \"book_id\": \"old_item\"})\n",
    "interactions_df[\"old_item\"] = interactions_df[\"old_item\"].astype(np.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "gibbs_sampling_iterations = 1\n",
    "nr_recommendations = 10\n",
    "alpha = 0.0002  # Learning rate\n",
    "\n",
    "hiddenUnits = 100\n",
    "# visibleUnits = shape[1]\n",
    "\n",
    "rng = tf.random.Generator.from_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c8049f1f65e4dd1852ae3a5f8e23423"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c052a333c9f64559b08f13fdaac1741b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64958bfe41b94ac08fdc5c0eaa307e0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1480 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8747403d14764fea9f8c86802ecc0663"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-55-b8d39d0cbd20>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0mhb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mVariable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrng\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mhiddenUnits\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m     \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_rbm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0;31m# recommendations = np.empty((shape[0], 5), dtype=np.int_)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-00d4ba64fb32>\u001B[0m in \u001B[0;36mtrain_rbm\u001B[0;34m(train, validation, weights, vb, hb, cond_weights)\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0mdelta\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtemp_0\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mtemp_k\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mv0_state\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0mweights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mweights\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0malpha\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mdelta\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0mvb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvb\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0malpha\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce_mean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv0_state\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mvk_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mhb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhb\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0malpha\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce_mean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mh0_state\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mhk_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mr_binary_op_wrapper\u001B[0;34m(y, x)\u001B[0m\n\u001B[1;32m   1416\u001B[0m       \u001B[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1417\u001B[0m       \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmaybe_promote_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1418\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1419\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1420\u001B[0m   \u001B[0;31m# Propagate func.__doc__ to the wrappers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36m_mul_dispatch\u001B[0;34m(x, y, name)\u001B[0m\n\u001B[1;32m   1745\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msparse_tensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSparseTensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnew_vals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1746\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1747\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mmultiply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1748\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1749\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mop_dispatch_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1088\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0miterable_params\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1089\u001B[0m           \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreplace_iterable_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterable_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1090\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mapi_dispatcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1091\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mNotImplemented\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1092\u001B[0m           \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def generate_matrix(fold_df: pd.DataFrame, interactions_df: pd.DataFrame, shape: tuple) -> sparse.csr_matrix:\n",
    "    temp = pd.merge(fold_df, interactions_df, how=\"left\", on=[\"old_user\", \"old_item\"])\n",
    "    rows = temp[\"user_id\"]\n",
    "    cols = temp[\"item_id\"]\n",
    "    ratings = temp[\"rating\"]\n",
    "    return sparse.csr_matrix((ratings, (rows, cols)), shape=shape)\n",
    "\n",
    "all_recommendations = []\n",
    "\n",
    "for fold_nr in trange(5, leave=False):\n",
    "\n",
    "    train_df = pd.read_csv(f\"data/train_fold{fold_nr}.csv\")\n",
    "    test_df = pd.read_csv(f\"data/test_fold{fold_nr}.csv\")\n",
    "\n",
    "    shape = (max(train_df[\"user_id\"].max(), test_df[\"user_id\"].max()) + 1,\n",
    "             max(train_df[\"item_id\"].max(), test_df[\"item_id\"].max()) + 1)\n",
    "    visibleUnits = shape[1]\n",
    "\n",
    "    train = generate_matrix(train_df, interactions_df, shape)\n",
    "    test = generate_matrix(test_df, interactions_df, shape)\n",
    "\n",
    "    tensor_train = convert_sparse_matrix_to_sparse_tensor(train)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(tensor_train).batch(batch_size)\n",
    "\n",
    "    W = tf.Variable(rng.normal([visibleUnits, hiddenUnits], 0, 0.1), tf.float32)\n",
    "    # D = tf.Variable(rng.normal([visibleUnits, hiddenUnits], 0, 0.1), tf.float32)\n",
    "    vb = tf.Variable(rng.normal([visibleUnits], 0, 0.1), tf.float32)\n",
    "    hb = tf.Variable(rng.normal([hiddenUnits], 0, 0.1), tf.float32)\n",
    "\n",
    "    W, _, vb, hb, _, _ = train_rbm(train_ds, None, W, vb, hb)\n",
    "\n",
    "    recommendations = np.empty((shape[0], 5), dtype=np.int_)\n",
    "    for batch_start in trange(0, train.shape[0], batch_size, leave=False):\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch = train[batch_start: batch_end]\n",
    "        recommendations[batch_start: batch_end] = get_k_recommendations(batch.toarray(), W, hb, vb, 5)\n",
    "\n",
    "    all_recommendations.append([recommendations])\n",
    "\n",
    "    recommendations = np.empty((shape[0], 10), dtype=np.int_)\n",
    "    for batch_start in trange(0, train.shape[0], batch_size, leave=False):\n",
    "        batch_end = batch_start + batch_size\n",
    "        batch = train[batch_start: batch_end]\n",
    "        recommendations[batch_start: batch_end] = get_k_recommendations(batch.toarray(), W, hb, vb, 10)\n",
    "\n",
    "    all_recommendations[-1].append(recommendations)\n",
    "    all_recommendations[-1].append(shape)\n",
    "    all_recommendations[-1] = tuple(all_recommendations[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for fold_nr, (recommendations_5, recommendations_10, shape) in enumerate(all_recommendations):\n",
    "    metrics.append({})\n",
    "\n",
    "    print(len(np.unique(recommendations_10)))\n",
    "    print(np.unique(recommendations_10))\n",
    "\n",
    "    test_df = pd.read_csv(f\"data/test_fold{fold_nr}.csv\")\n",
    "    test = generate_matrix(test_df, interactions_df, shape)\n",
    "\n",
    "    metrics[-1][\"recall@5\"] = compute_recall(test, array_to_sparse_matrix(recommendations_5))\n",
    "    tqdm.write(f\"Recall @ 5 = {metrics[-1]['recall@5']}\")\n",
    "\n",
    "    metrics[-1][\"ncdg@5\"] = compute_ndcg(test, recommendations_5)\n",
    "    tqdm.write(f\"NCDG @ 5 = {metrics[-1]['ncdg@5']}\")\n",
    "\n",
    "    metrics[-1][\"recall@10\"] = compute_recall(test, array_to_sparse_matrix(recommendations_10))\n",
    "    tqdm.write(f\"Recall @ 10 = {metrics[-1]['recall@10']}\")\n",
    "\n",
    "    metrics[-1][\"ncdg@10\"] = compute_ndcg(test, recommendations_10)\n",
    "    tqdm.write(f\"NCDG @ 10 = {metrics[-1]['ncdg@10']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}